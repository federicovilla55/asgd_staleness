{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQ+mJ6AZoTfuczUX+i+Ojk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/federicovilla55/optML_mini_project/blob/setup/Simple_MLP_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5KiZNmrzre_",
        "outputId": "51f1c9ce-37d9-4a93-ecce-e82613cdbbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openml in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
            "Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from openml) (2.5.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.11/dist-packages (from openml) (0.14.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from openml) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from openml) (1.6.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from openml) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from openml) (2.2.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.11/dist-packages (from openml) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from openml) (2.0.2)\n",
            "Requirement already satisfied: minio in /usr/local/lib/python3.11/dist-packages (from openml) (7.2.15)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from openml) (18.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openml) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from openml) (24.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->openml) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->openml) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->openml) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->openml) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->openml) (3.6.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from minio->openml) (2025.1.31)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from minio->openml) (2.3.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from minio->openml) (23.1.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.11/dist-packages (from minio->openml) (3.22.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from minio->openml) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->openml) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->openml) (3.10)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->minio->openml) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio->openml) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio->openml) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install openml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from getpass import getpass\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "aJYHzU_AAhji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONNECT TO GIT**\n",
        "\n",
        "You will need to create and store a Personal Access Token PAT and use this to connect with git. (this takes 30 seconds to create)"
      ],
      "metadata": {
        "id": "AuwIlYgGHC81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = getpass(\"Enter your GitHub token: \")\n",
        "repo_url = f\"https://{token}@github.com/federicovilla55/optML_mini_project.git\"\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd optML_mini_project\n",
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn-IA7KzHCDI",
        "outputId": "6c788add-a66b-4526-baa9-2d274ea18ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub token: ··········\n",
            "Cloning into 'optML_mini_project'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 7 (delta 0), reused 7 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (7/7), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where to work in git"
      ],
      "metadata": {
        "id": "4xvt7UdiLGY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To create a branch:\n",
        "\n",
        "#!git checkout -b branch_name\n",
        "\n",
        "# Switch to an existing branch:\n",
        "\n",
        "#!git checkout existing_branch_name\n",
        "\n",
        "#List all branches:\n",
        "\n",
        "#!git branch\n",
        "\n",
        "#Push a new branch to GitHub (if needed):\n",
        "\n",
        "#!git push -u origin branch_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auAR553tLOMS",
        "outputId": "939addb6-8825-48f0-ceca-294b8c38b7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 0 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "remote: \n",
            "remote: Create a pull request for 'setup' on GitHub by visiting:\u001b[K\n",
            "remote:      https://github.com/federicovilla55/optML_mini_project/pull/new/setup\u001b[K\n",
            "remote: \n",
            "To https://github.com/federicovilla55/optML_mini_project.git\n",
            " * [new branch]      setup -> setup\n",
            "Branch 'setup' set up to track remote branch 'setup' from 'origin'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this add the end to push it to git"
      ],
      "metadata": {
        "id": "lGko8bZxMwak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va_MLakjOJSF",
        "outputId": "f1b28fb9-c747-4d4e-e40c-184eccf89ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Mar 31 17:03 .\n",
            "drwxr-xr-x 1 root root 4096 Mar 31 15:34 ..\n",
            "drwxr-xr-x 4 root root 4096 Mar 24 13:34 .config\n",
            "drwxr-xr-x 3 root root 4096 Mar 31 17:03 optML_mini_project\n",
            "drwxr-xr-x 1 root root 4096 Mar 24 13:34 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/Simple_MLP_setup.ipynb /content/optML_mini_project/\n",
        "%cd /content/optML_mini_project\n",
        "!ls -la\n",
        "!git add Simple_MLP_setup.ipynb\n",
        "!git commit -m \"test\"\n",
        "!git push"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQfvfZ8SK6Wn",
        "outputId": "141354f7-fb7b-4ad5-f857-6d949143833c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/Simple_MLP_setup.ipynb': No such file or directory\n",
            "/content/optML_mini_project\n",
            "total 20\n",
            "drwxr-xr-x 3 root root 4096 Mar 31 17:03 .\n",
            "drwxr-xr-x 1 root root 4096 Mar 31 17:03 ..\n",
            "drwxr-xr-x 8 root root 4096 Mar 31 17:17 .git\n",
            "-rw-r--r-- 1 root root  119 Mar 31 17:03 project.ipynb\n",
            "-rw-r--r-- 1 root root  230 Mar 31 17:03 README.md\n",
            "fatal: pathspec 'Simple_MLP_setup.ipynb' did not match any files\n",
            "On branch setup\n",
            "Your branch is up to date with 'origin/setup'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA SET**\n",
        "\n",
        "UCI Adult dataset (also known as the \"Census Income\" dataset). This dataset is widely used for binary classification tasks and contains demographic information (such as age, work class, education, marital status, etc.) with the goal of predicting whether an individual earns more than $50K per year.\n",
        "\n",
        "Size: Originally about 48,842 instances; after cleaning, around 45,000 instances.\n",
        "\n",
        "Features: 14 demographic/categorical features (after one-hot encoding) and standardized numerical features."
      ],
      "metadata": {
        "id": "R5HOVxgS5WOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# 1. Load and preprocess the UCI Adult dataset\n",
        "adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
        "df = adult.frame\n",
        "\n",
        "# Drop rows with missing values and separate features and target.\n",
        "df = df.replace('?', np.nan).dropna()\n",
        "y = df['class'].apply(lambda x: 1 if x.strip() == '>50K' else 0).values\n",
        "X = df.drop(columns=['class'])\n",
        "\n",
        "# One-hot encode categorical variables.\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# Standardize numerical features.\n",
        "scaler = StandardScaler()\n",
        "X[X.columns] = scaler.fit_transform(X[X.columns])\n",
        "\n",
        "# Convert to numpy arrays.\n",
        "X_np = X.values.astype(np.float32)\n",
        "y_np = y.astype(np.int64)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%).\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=seed, stratify=y_np)\n",
        "\n",
        "# Create TensorDatasets and DataLoaders.\n",
        "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "#    The input dimension is determined by the number of features in the preprocessed data.\n",
        "input_dim = X_np.shape[1]\n",
        "num_classes = 2  # binary classification"
      ],
      "metadata": {
        "id": "P3PDfjuq15N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that creates the architecture"
      ],
      "metadata": {
        "id": "4eqw5jNh84p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a helper function to get the activation function\n",
        "def get_activation_fn(name):\n",
        "    if name.lower() == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif name.lower() == 'sigmoid':\n",
        "        return nn.Sigmoid()\n",
        "    elif name.lower() == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported activation function. Choose from 'relu', 'sigmoid', or 'tanh'.\")\n",
        "\n",
        "# Customizable MLP Model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, amount_layers=2, neurons_per_layer=256,\n",
        "                 activation='relu', dropout_rate=0.0):\n",
        "        \"\"\"\n",
        "        input_dim: Number of input features.\n",
        "        num_classes: Number of classes in the output.\n",
        "        amount_layers: Number of hidden layers.\n",
        "        neurons_per_layer: Number of neurons in each hidden layer.\n",
        "        activation: Activation function to use ('relu', 'sigmoid', or 'tanh').\n",
        "        dropout_rate: Dropout rate to apply after each hidden layer.\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        self.activation = get_activation_fn(activation)\n",
        "\n",
        "        # First hidden layer: from input_dim to neurons_per_layer.\n",
        "        self.hidden_layers.append(nn.Linear(input_dim, neurons_per_layer))\n",
        "\n",
        "        # Additional hidden layers.\n",
        "        for _ in range(amount_layers - 1):\n",
        "            self.hidden_layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
        "\n",
        "        # Output layer.\n",
        "        self.output_layer = nn.Linear(neurons_per_layer, num_classes)\n",
        "\n",
        "        # Optional dropout layer.\n",
        "        self.use_dropout = dropout_rate > 0\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through each hidden layer with activation and optional dropout.\n",
        "        for layer in self.hidden_layers:\n",
        "            x = self.activation(layer(x))\n",
        "            if self.use_dropout:\n",
        "                x = self.dropout(x)\n",
        "        # Output layer (no activation applied; use appropriate loss later).\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Bx8BU5J92lkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define the training function.\n",
        "def train_model(model, optimizer, train_loader, grad_noise_std=0.0):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()   #Loss function\n",
        "    running_loss = 0.0\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        # Optionally inject gradient noise\n",
        "        if grad_noise_std > 0.0:\n",
        "            with torch.no_grad():\n",
        "                for param in model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        noise = torch.randn_like(param.grad) * grad_noise_std\n",
        "                        param.grad.add_(noise)\n",
        "\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "    avg_loss = running_loss / len(train_loader.dataset)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "_C9n4I_L2pGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Evaluate the model on the test data.\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += (pred == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "Ho-1m_Dn3DE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Run cross-validation experiment.\n",
        "def run_experiment(name, train_loader, neurons_per_layer, amount_layers, activation, learning_rate, epochs, dropout_rate=0.0, weight_decay=0.0, grad_noise_std=0.0, use_asgd=False, num_workers=0):\n",
        "    print(f\"\\nStarting experiment: {name}\")\n",
        "    model = MLP(input_dim, num_classes, amount_layers, neurons_per_layer, activation, dropout_rate).to(device)\n",
        "    train_losses = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    # Choose optimizer.\n",
        "    if use_asgd:\n",
        "        optimizer = optim.ASGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss = train_model(model, optimizer, train_loader, grad_noise_std=grad_noise_std)\n",
        "        train_losses.append(train_loss)\n",
        "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # 4. Plot the training loss after each epoch.\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epochs + 1), train_losses, marker='o')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Training Loss\")\n",
        "    plt.title(\"Training Loss vs. Epoch\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "vdp4YPtu3Iqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPERIMENTS**"
      ],
      "metadata": {
        "id": "TjFNyAvk3b6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for architecture\n",
        "amount_layers = 3\n",
        "neurons_per_layer = 128\n",
        "activation = 'relu'       # choose 'relu', 'sigmoid', or 'tanh'\n"
      ],
      "metadata": {
        "id": "_IaDpsAV8YKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General Hyperparameters for training\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "B-j-sTII7s8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1: Baseline (no explicit regularization)\n",
        "\n",
        "baseline_trained_model = run_experiment(\"Baseline\", train_loader, neurons_per_layer, amount_layers, activation, learning_rate, epochs, dropout_rate=0.0, weight_decay=0.0, grad_noise_std=0.0, use_asgd=False, num_workers=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "wyLpzpEv3SwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2: Dropout Regularization\n",
        "\n",
        "dropout_trained_model = run_experiment(\"Dropout\", train_loader, neurons_per_layer, amount_layers, activation, learning_rate, epochs, dropout_rate=0.5, weight_decay=0.0, grad_noise_std=0.0, use_asgd=False, num_workers=0)\n"
      ],
      "metadata": {
        "id": "c_A67csL3aE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3: Weight Decay (L2 Regularization)\n",
        "\n",
        "weight_decay_trained_model = run_experiment(\"Weight Decay\", train_loader, neurons_per_layer, amount_layers, activation, learning_rate, epochs, dropout_rate=0.0, weight_decay=1e-4, grad_noise_std=0.0, use_asgd=False, num_workers=0)\n"
      ],
      "metadata": {
        "id": "zZm2DPEl3vZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 4: Gradient Noise Injection\n",
        "\n",
        "grad_noise_trained_model = run_experiment(\"Gradient Noise\", train_loader, neurons_per_layer, amount_layers, activation, learning_rate, epochs, dropout_rate=0.0, weight_decay=0.0, grad_noise_std=0.01, use_asgd=False, num_workers=0)\n"
      ],
      "metadata": {
        "id": "-tEzV-2K303E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 5: ASGD with no explicit regularization and 2 DataLoader workers.\n",
        "\n",
        "asgd_trained_model = run_experiment(\"ASGD (2 workers)\", train_loader, neurons_per_layer, amount_layers, activation, learning_rate, epochs, dropout_rate=0.0, weight_decay=0.0, grad_noise_std=0.0, use_asgd=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "PX9sgC4J3-C4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}