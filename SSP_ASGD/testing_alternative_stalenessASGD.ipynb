{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing as mp\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConfigParameters:\n",
    "    \"\"\"\n",
    "    Configuration for Stale Synchronous Parallel training for Asynchronous SGD (SSP-ASGD).\n",
    "\n",
    "    :param num_workers: Number of worker processes.\n",
    "    :param staleness: Staleness bound allowed for the workers during training.\n",
    "    :param lr: Learning rate for the model.\n",
    "    :param local_steps: Number of local updates per worker.\n",
    "    :param batch_size: Batch size for each training step.\n",
    "    :param device: Device to use for training.\n",
    "    :param log_level: Logging verbosity level.\n",
    "    \"\"\"\n",
    "    num_workers: int = 4\n",
    "    staleness: int = 2\n",
    "    lr: float  = 0.01\n",
    "    local_steps: int = 500\n",
    "    batch_size: int = 128\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    log_level: int = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78808d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterServer:\n",
    "    \"\"\"\n",
    "    Parameter Server for SSP-ASGD with immediate, per-worker updates.\n",
    "    Each pushed gradient is applied right away (divided by num_workers),\n",
    "    global version is updated to max(t_max, version), and per-worker versions\n",
    "    are recorded.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, param: ConfigParameters) -> None:\n",
    "        self.param = param\n",
    "        # Shared global parameters\n",
    "        self.theta = [p.detach() for p in model.parameters()]\n",
    "        for p in self.theta:\n",
    "            p.share_memory_()\n",
    "\n",
    "        # Synchronization primitives\n",
    "        self._lock = mp.Lock()\n",
    "        self._cv = mp.Condition(self._lock)\n",
    "\n",
    "        # Global version and per-worker versions\n",
    "        self._current_version = mp.Value(\"i\", 0)\n",
    "        self._worker_versions = mp.Array(\"i\", [0] * param.num_workers)\n",
    "\n",
    "    def pull(self) -> Tuple[list[torch.Tensor], int]:\n",
    "        \"\"\"\n",
    "        Return a clone of the current global parameters and the current global version.\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            return [p.clone() for p in self.theta], self._current_version.value\n",
    "\n",
    "    def push(self, wid: int, version: int, grads: list[torch.Tensor]) -> None:\n",
    "        \"\"\"\n",
    "        Apply the worker's gradient update immediately, dividing by num_workers,\n",
    "        update the global version to max(current, version), and record the worker's version.\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            scale = self.param.lr / self.param.num_workers\n",
    "            for idx, g in enumerate(grads):\n",
    "                self.theta[idx].sub_(scale * g.to(self.theta[idx].device))\n",
    "\n",
    "            # Update global version\n",
    "            if version > self._current_version.value:\n",
    "                self._current_version.value = version\n",
    "\n",
    "            # Record this worker's version\n",
    "            self._worker_versions[wid] = version\n",
    "\n",
    "            # Notify any waiting pulls\n",
    "            self._cv.notify_all()\n",
    "\n",
    "    def get_version(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the current global version.\n",
    "        \"\"\"\n",
    "        return self._current_version.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c49f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(\n",
    "    w_id: int,\n",
    "    server:  ParameterServer,\n",
    "    model_fn: Callable[[int], nn.Module],\n",
    "    input_dim:  int,\n",
    "    dataset_builder: Callable[[int,int,int], Tuple[torch.utils.data.DataLoader,int]],\n",
    "    param: ConfigParameters\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Worker process for SSP-ASGD.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=param.log_level,\n",
    "        format=f\"%(asctime)s [Worker-{w_id}] %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    # Build data loader\n",
    "    loader, _ = dataset_builder(param.num_workers, param.batch_size, w_id)\n",
    "    device = torch.device(param.device)\n",
    "    model = model_fn(input_dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Initial pull\n",
    "    state, version = server.pull()\n",
    "    with torch.no_grad():\n",
    "        for p, s in zip(model.parameters(), state):\n",
    "            p.copy_(s.to(device))\n",
    "    local_ver = version\n",
    "\n",
    "    # Local training loop\n",
    "    for _ in range(param.local_steps):\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            model.train()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch.float())\n",
    "            loss.backward()\n",
    "\n",
    "            # Collect gradients\n",
    "            grads = [p.grad.detach().cpu() for p in model.parameters()]\n",
    "            for p in model.parameters():\n",
    "                p.grad = None\n",
    "\n",
    "            # Pull if too stale\n",
    "            t_max = server.get_version()\n",
    "            if t_max - local_ver > param.staleness:\n",
    "                state, t_max = server.pull()\n",
    "                with torch.no_grad():\n",
    "                    for p, s in zip(model.parameters(), state):\n",
    "                        p.copy_(s.to(device))\n",
    "                local_ver = t_max\n",
    "\n",
    "            # Advance version and push update\n",
    "            local_ver += 1\n",
    "            server.push(w_id, local_ver, grads)\n",
    "\n",
    "\n",
    "def run_ssp_training(\n",
    "    dataset_builder: Callable[[int,int,int], Tuple[torch.utils.data.DataLoader,int]],\n",
    "    model_fn: Callable[[int], nn.Module],\n",
    "    param: ConfigParameters = ConfigParameters(),\n",
    ") -> Tuple[list[torch.Tensor], int]:\n",
    "    \"\"\"\n",
    "    Run SSP-ASGD training across multiple worker processes.\n",
    "\n",
    "    Returns the final global parameters and the input dimension.\n",
    "    \"\"\"\n",
    "    _, input_dim = dataset_builder(param.num_workers, param.batch_size, 0)\n",
    "    init_model = model_fn(input_dim)\n",
    "    ps = ParameterServer(init_model, param)\n",
    "\n",
    "    ctx = mp.get_context(\"fork\")\n",
    "    procs = []\n",
    "    for wid in range(param.num_workers):\n",
    "        p = ctx.Process(\n",
    "            target=worker,\n",
    "            args=(wid, ps, model_fn, input_dim, dataset_builder, param),\n",
    "            daemon=False\n",
    "        )\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "\n",
    "    for p in procs:\n",
    "        p.join()\n",
    "        if p.exitcode != 0:\n",
    "            raise RuntimeError(f\"Worker {p.name} crashed (exitcode {p.exitcode})\")\n",
    "\n",
    "    theta, _ = ps.pull()\n",
    "    return theta, input_dim\n",
    "\n",
    "\n",
    "def build_model(theta: list[torch.Tensor], model_fn: Callable[[int], nn.Module], input_dim: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Instantiate a model and load the given parameters.\n",
    "    \"\"\"\n",
    "    model = model_fn(input_dim)\n",
    "    with torch.no_grad():\n",
    "        for p, t in zip(model.parameters(), theta):\n",
    "            p.copy_(t)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(name: str, model: nn.Module, X_eval: np.ndarray, y_eval: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate binary classification model accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.from_numpy(X_eval)).numpy() > 0.5\n",
    "        acc = np.mean(preds == y_eval)\n",
    "        print(f\"{name} Test accuracy: {acc:.4f}\")\n",
    "        return acc\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
