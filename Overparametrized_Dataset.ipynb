{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/federicovilla55/optML_mini_project/blob/main/Overparametrized_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NO000Jlxam4"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eT0VZ51xam5",
        "outputId": "281b1637-1180-457f-d996-20faeb23dee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openml\n",
            "  Downloading openml-0.15.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting liac-arff>=2.4.0 (from openml)\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xmltodict (from openml)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from openml) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from openml) (1.6.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from openml) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from openml) (2.2.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.11/dist-packages (from openml) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from openml) (2.0.2)\n",
            "Collecting minio (from openml)\n",
            "  Downloading minio-7.2.15-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from openml) (18.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openml) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from openml) (24.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->openml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->openml) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->openml) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->openml) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->openml) (3.6.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from minio->openml) (2025.1.31)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from minio->openml) (2.3.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from minio->openml) (23.1.0)\n",
            "Collecting pycryptodome (from minio->openml)\n",
            "  Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from minio->openml) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->openml) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->openml) (3.10)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->minio->openml) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio->openml) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio->openml) (2.22)\n",
            "Downloading openml-0.15.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading minio-7.2.15-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11717 sha256=b990f9436992b8bc03a5b3de06de1eeccfd1a5afda37ed2c52185335325aacde\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/23/31/5e562fce1f95aabe57f2a7320d07433ba1cd152bcde2f6a002\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: xmltodict, pycryptodome, liac-arff, minio, openml\n",
            "Successfully installed liac-arff-2.5.0 minio-7.2.15 openml-0.15.1 pycryptodome-3.22.0 xmltodict-0.14.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aJYHzU_AAhji"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from getpass import getpass\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GIT"
      ],
      "metadata": {
        "id": "5GaZmm8oxiek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"caspar.amery@gmail.com\"\n",
        "!git config --global user.name \"casparamery\""
      ],
      "metadata": {
        "id": "-WmfriwCxk2L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = getpass(\"Enter your GitHub token: \")\n",
        "repo_url = f\"https://{token}@github.com/federicovilla55/optML_mini_project.git\"\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd optML_mini_project\n",
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9SY0W5yxsx6",
        "outputId": "7bad8932-7405-4ac1-a1ff-24069ff314b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub token: ··········\n",
            "Cloning into 'optML_mini_project'...\n",
            "remote: Enumerating objects: 289, done.\u001b[K\n",
            "remote: Counting objects: 100% (289/289), done.\u001b[K\n",
            "remote: Compressing objects: 100% (199/199), done.\u001b[K\n",
            "remote: Total 289 (delta 100), reused 270 (delta 86), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (289/289), 7.02 MiB | 25.15 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n",
            "/content/optML_mini_project\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To create a branch:\n",
        "\n",
        "#!git checkout -b branch_name\n",
        "\n",
        "# Switch to an existing branch:\n",
        "\n",
        "!git checkout main\n",
        "\n",
        "#List all branches:\n",
        "\n",
        "#!git branch\n",
        "\n",
        "#Push a new branch to GitHub (if needed):\n",
        "\n",
        "#!git push -u origin branch_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGg9RG5ixuyr",
        "outputId": "6a6f098f-7412-457b-b41f-1e8c9e32294d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmt8xFDPxam7"
      },
      "source": [
        "# Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# OVERPARAMETERIZED DATASETS WITH FEATURE BOUNDS [-3, 3]\n",
        "#\n",
        "# Features X are sampled uniformly in [-3, 3], ensuring all inputs\n",
        "# lie within the specified bounds before any polynomial transformations.\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def create_linear_dataset(n_samples=200, n_features=300, noise=0.0, random_state=None): #Currently noise set to 0\n",
        "    \"\"\"\n",
        "    Overparameterized linear regression dataset:\n",
        "      - X sampled U(-3, 3)\n",
        "      - y = X @ w_true + noise\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    X = rng.uniform(low=-3, high=3, size=(n_samples, n_features))\n",
        "    w_true = rng.randn(n_features)\n",
        "    y = X.dot(w_true) + noise * rng.randn(n_samples)\n",
        "    return X, y\n",
        "\n",
        "def create_poly_varied_dataset(n_samples=200, n_features=300,\n",
        "                               max_degree=4, noise=0.0, random_state=None): #Currently noise set to 0\n",
        "    \"\"\"\n",
        "    Overparameterized nonlinear regression dataset:\n",
        "      - X sampled U(-3, 3)\n",
        "      - Each feature i raised to its own degree_i ∈ [1, max_degree]\n",
        "      - y = sum_i w_true[i] * (X[:,i] ** degree_i) + noise\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    X = rng.uniform(low=-3, high=3, size=(n_samples, n_features))\n",
        "    w_true = rng.randn(n_features)\n",
        "    degrees = rng.randint(1, max_degree + 1, size=n_features)\n",
        "    X_pow = np.zeros_like(X)\n",
        "    for i, d in enumerate(degrees):\n",
        "        X_pow[:, i] = X[:, i] ** d\n",
        "    y = X_pow.dot(w_true) + noise * rng.randn(n_samples)\n",
        "    return X, y, degrees\n",
        "\n",
        "def split_data(X, y, val_size=0.2, test_size=0.2, random_state=None):\n",
        "    \"\"\"\n",
        "    Splits data into train (60%), validation (20%), and test (20%) sets.\n",
        "    \"\"\"\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state)\n",
        "    val_rel = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_rel, random_state=random_state)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "# ------------------------------\n",
        "# Example: generate & split both\n",
        "# ------------------------------\n",
        "# Linear dataset\n",
        "X_lin, y_lin = create_linear_dataset(random_state=42)\n",
        "lin_splits = split_data(X_lin, y_lin, val_size=0.2, test_size=0.2, random_state=42)\n",
        "print(\"Linear shapes:\", [arr.shape for arr in lin_splits])\n",
        "\n",
        "# Polynomial-varied dataset\n",
        "X_poly, y_poly, degrees = create_poly_varied_dataset(\n",
        "    n_samples=200, n_features=500, max_degree=4, noise=0.1, random_state=42)\n",
        "poly_splits = split_data(X_poly, y_poly, val_size=0.2, test_size=0.2, random_state=42)\n",
        "print(\"Poly-varied shapes:\", [arr.shape for arr in poly_splits])\n",
        "print(\"Sample feature degrees:\", degrees[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM37gycl5blM",
        "outputId": "42cfa8de-2e59-4476-fe6f-be95f4c84d4b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear shapes: [(120, 300), (120,), (40, 300), (40,), (40, 300), (40,)]\n",
            "Poly-varied shapes: [(120, 500), (120,), (40, 500), (40,), (40, 500), (40,)]\n",
            "Sample feature degrees: [2 3 4 1 1 3 4 4 4 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGtfWTnpcYzJ"
      },
      "source": [
        "# **Linear Case Experiments:**\n",
        "\n",
        "  Gradient Descent convergence\n",
        "\n",
        "Gradient descent on a convex quadratic converges to the global minimizer (here a zero‐loss solution) as long as you pick a step‐size\n",
        "$$\n",
        "\\eta < \\frac{2}{\\lambda_{\\max}(X^T X)},\n",
        "$$\n",
        "where $\\lambda_{\\max}(X^T X)$ is the largest eigenvalue of $(X^T X)$.\n",
        "In the limit of small enough\n",
        "𝜂\n",
        "η and enough iterations (full‐batch GD), you will drive the training loss to (arbitrarily close to) zero.\n",
        "\n",
        "  Training + evaluating all the different kind of regularization\n",
        "\n",
        "  - Baseline\n",
        "  - Dropout\n",
        "  - Weight Decay/L2\n",
        "  - Gradient Noise Injection\n",
        "  - ...."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Base line"
      ],
      "metadata": {
        "id": "gIqeF9dEGG2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --------------------\n",
        "# 1. Prepare datasets\n",
        "# --------------------\n",
        "# Assume lin_splits = (X_train_lin, y_train_lin, X_val_lin, y_val_lin, X_test_lin, y_test_lin)\n",
        "X_train_lin, y_train_lin, X_val_lin, y_val_lin, X_test_lin, y_test_lin = lin_splits\n",
        "\n",
        "# Combine training + validation\n",
        "X_comb = np.vstack([X_train_lin, X_val_lin])\n",
        "y_comb = np.concatenate([y_train_lin, y_val_lin])\n",
        "\n",
        "# TensorDatasets and DataLoaders\n",
        "train_ds = TensorDataset(torch.from_numpy(X_comb).float(), torch.from_numpy(y_comb).float())\n",
        "test_ds  = TensorDataset(torch.from_numpy(X_test_lin).float(), torch.from_numpy(y_test_lin).float())\n",
        "\n",
        "batch_size = 20  # 10% of 200 samples\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --------------------------------------\n",
        "# 2. Compute 95% of max GD step size\n",
        "# --------------------------------------\n",
        "U, S, Vt = np.linalg.svd(X_comb, full_matrices=False)\n",
        "lambda_max = S[0]**2\n",
        "eta_max = 2.0 / lambda_max\n",
        "eta_95 = 0.95 * eta_max\n",
        "\n",
        "# ------------------------\n",
        "# 3. Build & move model\n",
        "# ------------------------\n",
        "# Assume LinearModel is defined (single nn.Linear)\n",
        "model = LinearModel(input_dim=X_comb.shape[1], bias=True)\n",
        "device = torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=eta_95)\n",
        "\n",
        "# --------------------------\n",
        "# 4. Training with early stop\n",
        "# --------------------------\n",
        "best_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "max_no_improve = 100\n",
        "epsilon = 1e-6  # threshold for \"zero\" loss\n",
        "\n",
        "for epoch in range(1, 10001):  # upper limit\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(Xb)\n",
        "        loss = nn.MSELoss()(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running += loss.item() * Xb.size(0)\n",
        "    train_loss = running / len(train_ds)\n",
        "    print(f\"Epoch {epoch}: Train MSE = {train_loss:.6f}\")\n",
        "\n",
        "    # early stopping checks\n",
        "    if train_loss + epsilon < best_loss:\n",
        "        best_loss = train_loss\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if best_loss < epsilon:\n",
        "        print(\"Reached near-zero training loss. Stopping.\")\n",
        "        break\n",
        "    if epochs_no_improve >= max_no_improve:\n",
        "        print(f\"No improvement in {max_no_improve} epochs. Stopping.\")\n",
        "        break\n",
        "\n",
        "# -----------------\n",
        "# 5. Evaluate test\n",
        "# -----------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0.0\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        total += nn.MSELoss(reduction='sum')(model(Xb), yb).item()\n",
        "    test_mse = total / len(test_ds)\n",
        "print(f\"Test MSE = {test_mse:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JUR0IvrJGFYb",
        "outputId": "d01f3c1e-384f-4334-c268-9125e6d6cbfd"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train MSE = 734.472389\n",
            "Epoch 2: Train MSE = 616.099586\n",
            "Epoch 3: Train MSE = 520.655483\n",
            "Epoch 4: Train MSE = 442.283527\n",
            "Epoch 5: Train MSE = 381.745501\n",
            "Epoch 6: Train MSE = 330.107794\n",
            "Epoch 7: Train MSE = 287.191673\n",
            "Epoch 8: Train MSE = 252.711433\n",
            "Epoch 9: Train MSE = 222.738113\n",
            "Epoch 10: Train MSE = 197.891939\n",
            "Epoch 11: Train MSE = 176.179690\n",
            "Epoch 12: Train MSE = 158.174577\n",
            "Epoch 13: Train MSE = 142.482528\n",
            "Epoch 14: Train MSE = 129.291167\n",
            "Epoch 15: Train MSE = 117.028850\n",
            "Epoch 16: Train MSE = 106.514644\n",
            "Epoch 17: Train MSE = 97.211128\n",
            "Epoch 18: Train MSE = 89.023488\n",
            "Epoch 19: Train MSE = 82.223701\n",
            "Epoch 20: Train MSE = 75.673010\n",
            "Epoch 21: Train MSE = 69.715409\n",
            "Epoch 22: Train MSE = 64.579544\n",
            "Epoch 23: Train MSE = 60.049172\n",
            "Epoch 24: Train MSE = 55.798031\n",
            "Epoch 25: Train MSE = 51.957812\n",
            "Epoch 26: Train MSE = 48.509376\n",
            "Epoch 27: Train MSE = 45.543588\n",
            "Epoch 28: Train MSE = 42.425095\n",
            "Epoch 29: Train MSE = 39.873135\n",
            "Epoch 30: Train MSE = 37.488111\n",
            "Epoch 31: Train MSE = 35.181140\n",
            "Epoch 32: Train MSE = 33.283204\n",
            "Epoch 33: Train MSE = 31.354963\n",
            "Epoch 34: Train MSE = 29.512053\n",
            "Epoch 35: Train MSE = 27.984479\n",
            "Epoch 36: Train MSE = 26.507487\n",
            "Epoch 37: Train MSE = 25.047206\n",
            "Epoch 38: Train MSE = 23.753538\n",
            "Epoch 39: Train MSE = 22.565666\n",
            "Epoch 40: Train MSE = 21.451543\n",
            "Epoch 41: Train MSE = 20.430973\n",
            "Epoch 42: Train MSE = 19.398755\n",
            "Epoch 43: Train MSE = 18.514433\n",
            "Epoch 44: Train MSE = 17.620255\n",
            "Epoch 45: Train MSE = 16.838080\n",
            "Epoch 46: Train MSE = 16.080652\n",
            "Epoch 47: Train MSE = 15.373809\n",
            "Epoch 48: Train MSE = 14.672284\n",
            "Epoch 49: Train MSE = 14.113744\n",
            "Epoch 50: Train MSE = 13.521477\n",
            "Epoch 51: Train MSE = 12.922690\n",
            "Epoch 52: Train MSE = 12.390581\n",
            "Epoch 53: Train MSE = 11.907077\n",
            "Epoch 54: Train MSE = 11.458008\n",
            "Epoch 55: Train MSE = 11.024868\n",
            "Epoch 56: Train MSE = 10.570447\n",
            "Epoch 57: Train MSE = 10.170960\n",
            "Epoch 58: Train MSE = 9.802671\n",
            "Epoch 59: Train MSE = 9.462400\n",
            "Epoch 60: Train MSE = 9.085675\n",
            "Epoch 61: Train MSE = 8.754699\n",
            "Epoch 62: Train MSE = 8.454410\n",
            "Epoch 63: Train MSE = 8.151879\n",
            "Epoch 64: Train MSE = 7.900974\n",
            "Epoch 65: Train MSE = 7.633741\n",
            "Epoch 66: Train MSE = 7.345071\n",
            "Epoch 67: Train MSE = 7.138102\n",
            "Epoch 68: Train MSE = 6.894522\n",
            "Epoch 69: Train MSE = 6.638204\n",
            "Epoch 70: Train MSE = 6.458999\n",
            "Epoch 71: Train MSE = 6.247667\n",
            "Epoch 72: Train MSE = 6.056126\n",
            "Epoch 73: Train MSE = 5.853453\n",
            "Epoch 74: Train MSE = 5.660113\n",
            "Epoch 75: Train MSE = 5.510529\n",
            "Epoch 76: Train MSE = 5.348764\n",
            "Epoch 77: Train MSE = 5.177994\n",
            "Epoch 78: Train MSE = 5.026321\n",
            "Epoch 79: Train MSE = 4.887026\n",
            "Epoch 80: Train MSE = 4.738692\n",
            "Epoch 81: Train MSE = 4.605739\n",
            "Epoch 82: Train MSE = 4.474964\n",
            "Epoch 83: Train MSE = 4.351287\n",
            "Epoch 84: Train MSE = 4.225792\n",
            "Epoch 85: Train MSE = 4.127362\n",
            "Epoch 86: Train MSE = 4.004113\n",
            "Epoch 87: Train MSE = 3.886897\n",
            "Epoch 88: Train MSE = 3.790167\n",
            "Epoch 89: Train MSE = 3.686385\n",
            "Epoch 90: Train MSE = 3.590889\n",
            "Epoch 91: Train MSE = 3.497304\n",
            "Epoch 92: Train MSE = 3.417224\n",
            "Epoch 93: Train MSE = 3.315448\n",
            "Epoch 94: Train MSE = 3.234754\n",
            "Epoch 95: Train MSE = 3.156177\n",
            "Epoch 96: Train MSE = 3.075357\n",
            "Epoch 97: Train MSE = 3.000222\n",
            "Epoch 98: Train MSE = 2.919404\n",
            "Epoch 99: Train MSE = 2.843523\n",
            "Epoch 100: Train MSE = 2.772823\n",
            "Epoch 101: Train MSE = 2.709042\n",
            "Epoch 102: Train MSE = 2.637986\n",
            "Epoch 103: Train MSE = 2.593364\n",
            "Epoch 104: Train MSE = 2.522337\n",
            "Epoch 105: Train MSE = 2.471780\n",
            "Epoch 106: Train MSE = 2.408197\n",
            "Epoch 107: Train MSE = 2.348296\n",
            "Epoch 108: Train MSE = 2.293548\n",
            "Epoch 109: Train MSE = 2.248970\n",
            "Epoch 110: Train MSE = 2.192632\n",
            "Epoch 111: Train MSE = 2.140455\n",
            "Epoch 112: Train MSE = 2.092967\n",
            "Epoch 113: Train MSE = 2.055367\n",
            "Epoch 114: Train MSE = 1.998759\n",
            "Epoch 115: Train MSE = 1.959070\n",
            "Epoch 116: Train MSE = 1.920568\n",
            "Epoch 117: Train MSE = 1.876834\n",
            "Epoch 118: Train MSE = 1.829971\n",
            "Epoch 119: Train MSE = 1.791356\n",
            "Epoch 120: Train MSE = 1.753099\n",
            "Epoch 121: Train MSE = 1.712383\n",
            "Epoch 122: Train MSE = 1.685237\n",
            "Epoch 123: Train MSE = 1.642213\n",
            "Epoch 124: Train MSE = 1.610678\n",
            "Epoch 125: Train MSE = 1.575180\n",
            "Epoch 126: Train MSE = 1.543496\n",
            "Epoch 127: Train MSE = 1.506990\n",
            "Epoch 128: Train MSE = 1.479806\n",
            "Epoch 129: Train MSE = 1.450070\n",
            "Epoch 130: Train MSE = 1.420930\n",
            "Epoch 131: Train MSE = 1.387715\n",
            "Epoch 132: Train MSE = 1.364711\n",
            "Epoch 133: Train MSE = 1.335499\n",
            "Epoch 134: Train MSE = 1.307562\n",
            "Epoch 135: Train MSE = 1.278176\n",
            "Epoch 136: Train MSE = 1.257836\n",
            "Epoch 137: Train MSE = 1.234249\n",
            "Epoch 138: Train MSE = 1.208794\n",
            "Epoch 139: Train MSE = 1.187073\n",
            "Epoch 140: Train MSE = 1.159202\n",
            "Epoch 141: Train MSE = 1.138411\n",
            "Epoch 142: Train MSE = 1.114391\n",
            "Epoch 143: Train MSE = 1.093808\n",
            "Epoch 144: Train MSE = 1.071631\n",
            "Epoch 145: Train MSE = 1.050952\n",
            "Epoch 146: Train MSE = 1.033091\n",
            "Epoch 147: Train MSE = 1.011741\n",
            "Epoch 148: Train MSE = 0.994618\n",
            "Epoch 149: Train MSE = 0.974870\n",
            "Epoch 150: Train MSE = 0.953916\n",
            "Epoch 151: Train MSE = 0.936869\n",
            "Epoch 152: Train MSE = 0.919130\n",
            "Epoch 153: Train MSE = 0.904744\n",
            "Epoch 154: Train MSE = 0.888077\n",
            "Epoch 155: Train MSE = 0.870354\n",
            "Epoch 156: Train MSE = 0.853312\n",
            "Epoch 157: Train MSE = 0.838800\n",
            "Epoch 158: Train MSE = 0.825778\n",
            "Epoch 159: Train MSE = 0.808124\n",
            "Epoch 160: Train MSE = 0.793832\n",
            "Epoch 161: Train MSE = 0.779000\n",
            "Epoch 162: Train MSE = 0.766421\n",
            "Epoch 163: Train MSE = 0.751831\n",
            "Epoch 164: Train MSE = 0.738216\n",
            "Epoch 165: Train MSE = 0.724289\n",
            "Epoch 166: Train MSE = 0.712192\n",
            "Epoch 167: Train MSE = 0.700028\n",
            "Epoch 168: Train MSE = 0.688083\n",
            "Epoch 169: Train MSE = 0.675689\n",
            "Epoch 170: Train MSE = 0.663880\n",
            "Epoch 171: Train MSE = 0.650646\n",
            "Epoch 172: Train MSE = 0.640826\n",
            "Epoch 173: Train MSE = 0.626705\n",
            "Epoch 174: Train MSE = 0.618634\n",
            "Epoch 175: Train MSE = 0.608766\n",
            "Epoch 176: Train MSE = 0.597854\n",
            "Epoch 177: Train MSE = 0.587032\n",
            "Epoch 178: Train MSE = 0.575831\n",
            "Epoch 179: Train MSE = 0.567980\n",
            "Epoch 180: Train MSE = 0.557430\n",
            "Epoch 181: Train MSE = 0.547382\n",
            "Epoch 182: Train MSE = 0.537239\n",
            "Epoch 183: Train MSE = 0.528713\n",
            "Epoch 184: Train MSE = 0.521229\n",
            "Epoch 185: Train MSE = 0.510498\n",
            "Epoch 186: Train MSE = 0.504080\n",
            "Epoch 187: Train MSE = 0.493750\n",
            "Epoch 188: Train MSE = 0.486313\n",
            "Epoch 189: Train MSE = 0.476741\n",
            "Epoch 190: Train MSE = 0.469783\n",
            "Epoch 191: Train MSE = 0.463635\n",
            "Epoch 192: Train MSE = 0.454320\n",
            "Epoch 193: Train MSE = 0.445386\n",
            "Epoch 194: Train MSE = 0.439188\n",
            "Epoch 195: Train MSE = 0.432763\n",
            "Epoch 196: Train MSE = 0.426110\n",
            "Epoch 197: Train MSE = 0.419607\n",
            "Epoch 198: Train MSE = 0.410757\n",
            "Epoch 199: Train MSE = 0.404730\n",
            "Epoch 200: Train MSE = 0.397954\n",
            "Epoch 201: Train MSE = 0.391773\n",
            "Epoch 202: Train MSE = 0.385876\n",
            "Epoch 203: Train MSE = 0.379453\n",
            "Epoch 204: Train MSE = 0.372048\n",
            "Epoch 205: Train MSE = 0.368884\n",
            "Epoch 206: Train MSE = 0.361272\n",
            "Epoch 207: Train MSE = 0.355179\n",
            "Epoch 208: Train MSE = 0.351450\n",
            "Epoch 209: Train MSE = 0.344000\n",
            "Epoch 210: Train MSE = 0.339801\n",
            "Epoch 211: Train MSE = 0.332742\n",
            "Epoch 212: Train MSE = 0.328698\n",
            "Epoch 213: Train MSE = 0.322964\n",
            "Epoch 214: Train MSE = 0.318400\n",
            "Epoch 215: Train MSE = 0.313235\n",
            "Epoch 216: Train MSE = 0.308724\n",
            "Epoch 217: Train MSE = 0.303572\n",
            "Epoch 218: Train MSE = 0.297715\n",
            "Epoch 219: Train MSE = 0.293637\n",
            "Epoch 220: Train MSE = 0.288540\n",
            "Epoch 221: Train MSE = 0.285658\n",
            "Epoch 222: Train MSE = 0.280012\n",
            "Epoch 223: Train MSE = 0.275995\n",
            "Epoch 224: Train MSE = 0.270998\n",
            "Epoch 225: Train MSE = 0.268365\n",
            "Epoch 226: Train MSE = 0.263673\n",
            "Epoch 227: Train MSE = 0.259765\n",
            "Epoch 228: Train MSE = 0.255480\n",
            "Epoch 229: Train MSE = 0.250842\n",
            "Epoch 230: Train MSE = 0.247797\n",
            "Epoch 231: Train MSE = 0.244597\n",
            "Epoch 232: Train MSE = 0.239735\n",
            "Epoch 233: Train MSE = 0.237065\n",
            "Epoch 234: Train MSE = 0.233340\n",
            "Epoch 235: Train MSE = 0.228955\n",
            "Epoch 236: Train MSE = 0.225543\n",
            "Epoch 237: Train MSE = 0.222132\n",
            "Epoch 238: Train MSE = 0.219260\n",
            "Epoch 239: Train MSE = 0.215922\n",
            "Epoch 240: Train MSE = 0.213369\n",
            "Epoch 241: Train MSE = 0.209907\n",
            "Epoch 242: Train MSE = 0.206312\n",
            "Epoch 243: Train MSE = 0.203873\n",
            "Epoch 244: Train MSE = 0.200257\n",
            "Epoch 245: Train MSE = 0.197167\n",
            "Epoch 246: Train MSE = 0.193735\n",
            "Epoch 247: Train MSE = 0.191358\n",
            "Epoch 248: Train MSE = 0.188425\n",
            "Epoch 249: Train MSE = 0.186296\n",
            "Epoch 250: Train MSE = 0.182416\n",
            "Epoch 251: Train MSE = 0.180225\n",
            "Epoch 252: Train MSE = 0.177953\n",
            "Epoch 253: Train MSE = 0.175210\n",
            "Epoch 254: Train MSE = 0.172247\n",
            "Epoch 255: Train MSE = 0.169637\n",
            "Epoch 256: Train MSE = 0.167950\n",
            "Epoch 257: Train MSE = 0.165260\n",
            "Epoch 258: Train MSE = 0.162692\n",
            "Epoch 259: Train MSE = 0.160123\n",
            "Epoch 260: Train MSE = 0.158157\n",
            "Epoch 261: Train MSE = 0.155353\n",
            "Epoch 262: Train MSE = 0.153204\n",
            "Epoch 263: Train MSE = 0.151336\n",
            "Epoch 264: Train MSE = 0.148807\n",
            "Epoch 265: Train MSE = 0.146295\n",
            "Epoch 266: Train MSE = 0.144843\n",
            "Epoch 267: Train MSE = 0.142834\n",
            "Epoch 268: Train MSE = 0.140609\n",
            "Epoch 269: Train MSE = 0.138826\n",
            "Epoch 270: Train MSE = 0.136191\n",
            "Epoch 271: Train MSE = 0.134591\n",
            "Epoch 272: Train MSE = 0.132858\n",
            "Epoch 273: Train MSE = 0.130482\n",
            "Epoch 274: Train MSE = 0.129016\n",
            "Epoch 275: Train MSE = 0.127221\n",
            "Epoch 276: Train MSE = 0.125504\n",
            "Epoch 277: Train MSE = 0.122922\n",
            "Epoch 278: Train MSE = 0.121696\n",
            "Epoch 279: Train MSE = 0.119701\n",
            "Epoch 280: Train MSE = 0.117993\n",
            "Epoch 281: Train MSE = 0.116185\n",
            "Epoch 282: Train MSE = 0.114565\n",
            "Epoch 283: Train MSE = 0.112873\n",
            "Epoch 284: Train MSE = 0.111630\n",
            "Epoch 285: Train MSE = 0.109911\n",
            "Epoch 286: Train MSE = 0.108177\n",
            "Epoch 287: Train MSE = 0.106488\n",
            "Epoch 288: Train MSE = 0.105662\n",
            "Epoch 289: Train MSE = 0.103988\n",
            "Epoch 290: Train MSE = 0.102178\n",
            "Epoch 291: Train MSE = 0.101082\n",
            "Epoch 292: Train MSE = 0.099181\n",
            "Epoch 293: Train MSE = 0.098273\n",
            "Epoch 294: Train MSE = 0.096656\n",
            "Epoch 295: Train MSE = 0.095104\n",
            "Epoch 296: Train MSE = 0.094197\n",
            "Epoch 297: Train MSE = 0.092791\n",
            "Epoch 298: Train MSE = 0.091351\n",
            "Epoch 299: Train MSE = 0.090157\n",
            "Epoch 300: Train MSE = 0.088800\n",
            "Epoch 301: Train MSE = 0.087516\n",
            "Epoch 302: Train MSE = 0.086396\n",
            "Epoch 303: Train MSE = 0.085192\n",
            "Epoch 304: Train MSE = 0.083969\n",
            "Epoch 305: Train MSE = 0.082882\n",
            "Epoch 306: Train MSE = 0.081768\n",
            "Epoch 307: Train MSE = 0.080708\n",
            "Epoch 308: Train MSE = 0.079393\n",
            "Epoch 309: Train MSE = 0.078387\n",
            "Epoch 310: Train MSE = 0.077202\n",
            "Epoch 311: Train MSE = 0.076040\n",
            "Epoch 312: Train MSE = 0.075168\n",
            "Epoch 313: Train MSE = 0.074151\n",
            "Epoch 314: Train MSE = 0.073065\n",
            "Epoch 315: Train MSE = 0.071910\n",
            "Epoch 316: Train MSE = 0.071121\n",
            "Epoch 317: Train MSE = 0.070183\n",
            "Epoch 318: Train MSE = 0.069199\n",
            "Epoch 319: Train MSE = 0.068174\n",
            "Epoch 320: Train MSE = 0.067165\n",
            "Epoch 321: Train MSE = 0.066365\n",
            "Epoch 322: Train MSE = 0.065404\n",
            "Epoch 323: Train MSE = 0.064514\n",
            "Epoch 324: Train MSE = 0.063870\n",
            "Epoch 325: Train MSE = 0.062742\n",
            "Epoch 326: Train MSE = 0.062045\n",
            "Epoch 327: Train MSE = 0.061096\n",
            "Epoch 328: Train MSE = 0.060313\n",
            "Epoch 329: Train MSE = 0.059402\n",
            "Epoch 330: Train MSE = 0.058602\n",
            "Epoch 331: Train MSE = 0.057731\n",
            "Epoch 332: Train MSE = 0.057008\n",
            "Epoch 333: Train MSE = 0.056135\n",
            "Epoch 334: Train MSE = 0.055564\n",
            "Epoch 335: Train MSE = 0.054722\n",
            "Epoch 336: Train MSE = 0.054124\n",
            "Epoch 337: Train MSE = 0.053315\n",
            "Epoch 338: Train MSE = 0.052560\n",
            "Epoch 339: Train MSE = 0.051815\n",
            "Epoch 340: Train MSE = 0.051049\n",
            "Epoch 341: Train MSE = 0.050430\n",
            "Epoch 342: Train MSE = 0.049820\n",
            "Epoch 343: Train MSE = 0.049093\n",
            "Epoch 344: Train MSE = 0.048517\n",
            "Epoch 345: Train MSE = 0.047786\n",
            "Epoch 346: Train MSE = 0.047126\n",
            "Epoch 347: Train MSE = 0.046416\n",
            "Epoch 348: Train MSE = 0.045817\n",
            "Epoch 349: Train MSE = 0.045222\n",
            "Epoch 350: Train MSE = 0.044524\n",
            "Epoch 351: Train MSE = 0.043999\n",
            "Epoch 352: Train MSE = 0.043440\n",
            "Epoch 353: Train MSE = 0.042910\n",
            "Epoch 354: Train MSE = 0.042276\n",
            "Epoch 355: Train MSE = 0.041742\n",
            "Epoch 356: Train MSE = 0.041122\n",
            "Epoch 357: Train MSE = 0.040558\n",
            "Epoch 358: Train MSE = 0.040012\n",
            "Epoch 359: Train MSE = 0.039467\n",
            "Epoch 360: Train MSE = 0.038986\n",
            "Epoch 361: Train MSE = 0.038489\n",
            "Epoch 362: Train MSE = 0.037927\n",
            "Epoch 363: Train MSE = 0.037500\n",
            "Epoch 364: Train MSE = 0.037019\n",
            "Epoch 365: Train MSE = 0.036488\n",
            "Epoch 366: Train MSE = 0.035962\n",
            "Epoch 367: Train MSE = 0.035509\n",
            "Epoch 368: Train MSE = 0.035005\n",
            "Epoch 369: Train MSE = 0.034517\n",
            "Epoch 370: Train MSE = 0.034160\n",
            "Epoch 371: Train MSE = 0.033677\n",
            "Epoch 372: Train MSE = 0.033297\n",
            "Epoch 373: Train MSE = 0.032768\n",
            "Epoch 374: Train MSE = 0.032358\n",
            "Epoch 375: Train MSE = 0.032022\n",
            "Epoch 376: Train MSE = 0.031433\n",
            "Epoch 377: Train MSE = 0.031065\n",
            "Epoch 378: Train MSE = 0.030680\n",
            "Epoch 379: Train MSE = 0.030186\n",
            "Epoch 380: Train MSE = 0.029823\n",
            "Epoch 381: Train MSE = 0.029394\n",
            "Epoch 382: Train MSE = 0.029076\n",
            "Epoch 383: Train MSE = 0.028606\n",
            "Epoch 384: Train MSE = 0.028324\n",
            "Epoch 385: Train MSE = 0.027884\n",
            "Epoch 386: Train MSE = 0.027599\n",
            "Epoch 387: Train MSE = 0.027275\n",
            "Epoch 388: Train MSE = 0.026702\n",
            "Epoch 389: Train MSE = 0.026554\n",
            "Epoch 390: Train MSE = 0.026190\n",
            "Epoch 391: Train MSE = 0.025852\n",
            "Epoch 392: Train MSE = 0.025488\n",
            "Epoch 393: Train MSE = 0.025143\n",
            "Epoch 394: Train MSE = 0.024860\n",
            "Epoch 395: Train MSE = 0.024457\n",
            "Epoch 396: Train MSE = 0.024094\n",
            "Epoch 397: Train MSE = 0.023822\n",
            "Epoch 398: Train MSE = 0.023485\n",
            "Epoch 399: Train MSE = 0.023219\n",
            "Epoch 400: Train MSE = 0.022871\n",
            "Epoch 401: Train MSE = 0.022625\n",
            "Epoch 402: Train MSE = 0.022366\n",
            "Epoch 403: Train MSE = 0.022083\n",
            "Epoch 404: Train MSE = 0.021769\n",
            "Epoch 405: Train MSE = 0.021492\n",
            "Epoch 406: Train MSE = 0.021171\n",
            "Epoch 407: Train MSE = 0.020866\n",
            "Epoch 408: Train MSE = 0.020554\n",
            "Epoch 409: Train MSE = 0.020392\n",
            "Epoch 410: Train MSE = 0.020090\n",
            "Epoch 411: Train MSE = 0.019852\n",
            "Epoch 412: Train MSE = 0.019561\n",
            "Epoch 413: Train MSE = 0.019318\n",
            "Epoch 414: Train MSE = 0.019064\n",
            "Epoch 415: Train MSE = 0.018773\n",
            "Epoch 416: Train MSE = 0.018583\n",
            "Epoch 417: Train MSE = 0.018320\n",
            "Epoch 418: Train MSE = 0.018133\n",
            "Epoch 419: Train MSE = 0.017826\n",
            "Epoch 420: Train MSE = 0.017584\n",
            "Epoch 421: Train MSE = 0.017372\n",
            "Epoch 422: Train MSE = 0.017134\n",
            "Epoch 423: Train MSE = 0.016974\n",
            "Epoch 424: Train MSE = 0.016781\n",
            "Epoch 425: Train MSE = 0.016491\n",
            "Epoch 426: Train MSE = 0.016273\n",
            "Epoch 427: Train MSE = 0.016080\n",
            "Epoch 428: Train MSE = 0.015818\n",
            "Epoch 429: Train MSE = 0.015633\n",
            "Epoch 430: Train MSE = 0.015476\n",
            "Epoch 431: Train MSE = 0.015271\n",
            "Epoch 432: Train MSE = 0.015051\n",
            "Epoch 433: Train MSE = 0.014868\n",
            "Epoch 434: Train MSE = 0.014693\n",
            "Epoch 435: Train MSE = 0.014496\n",
            "Epoch 436: Train MSE = 0.014311\n",
            "Epoch 437: Train MSE = 0.014112\n",
            "Epoch 438: Train MSE = 0.013980\n",
            "Epoch 439: Train MSE = 0.013779\n",
            "Epoch 440: Train MSE = 0.013589\n",
            "Epoch 441: Train MSE = 0.013429\n",
            "Epoch 442: Train MSE = 0.013247\n",
            "Epoch 443: Train MSE = 0.013058\n",
            "Epoch 444: Train MSE = 0.012930\n",
            "Epoch 445: Train MSE = 0.012728\n",
            "Epoch 446: Train MSE = 0.012622\n",
            "Epoch 447: Train MSE = 0.012436\n",
            "Epoch 448: Train MSE = 0.012275\n",
            "Epoch 449: Train MSE = 0.012098\n",
            "Epoch 450: Train MSE = 0.011955\n",
            "Epoch 451: Train MSE = 0.011784\n",
            "Epoch 452: Train MSE = 0.011618\n",
            "Epoch 453: Train MSE = 0.011511\n",
            "Epoch 454: Train MSE = 0.011374\n",
            "Epoch 455: Train MSE = 0.011197\n",
            "Epoch 456: Train MSE = 0.011046\n",
            "Epoch 457: Train MSE = 0.010895\n",
            "Epoch 458: Train MSE = 0.010764\n",
            "Epoch 459: Train MSE = 0.010613\n",
            "Epoch 460: Train MSE = 0.010501\n",
            "Epoch 461: Train MSE = 0.010418\n",
            "Epoch 462: Train MSE = 0.010233\n",
            "Epoch 463: Train MSE = 0.010105\n",
            "Epoch 464: Train MSE = 0.009987\n",
            "Epoch 465: Train MSE = 0.009837\n",
            "Epoch 466: Train MSE = 0.009740\n",
            "Epoch 467: Train MSE = 0.009596\n",
            "Epoch 468: Train MSE = 0.009483\n",
            "Epoch 469: Train MSE = 0.009337\n",
            "Epoch 470: Train MSE = 0.009240\n",
            "Epoch 471: Train MSE = 0.009141\n",
            "Epoch 472: Train MSE = 0.008991\n",
            "Epoch 473: Train MSE = 0.008870\n",
            "Epoch 474: Train MSE = 0.008775\n",
            "Epoch 475: Train MSE = 0.008679\n",
            "Epoch 476: Train MSE = 0.008558\n",
            "Epoch 477: Train MSE = 0.008441\n",
            "Epoch 478: Train MSE = 0.008335\n",
            "Epoch 479: Train MSE = 0.008223\n",
            "Epoch 480: Train MSE = 0.008134\n",
            "Epoch 481: Train MSE = 0.008035\n",
            "Epoch 482: Train MSE = 0.007932\n",
            "Epoch 483: Train MSE = 0.007816\n",
            "Epoch 484: Train MSE = 0.007726\n",
            "Epoch 485: Train MSE = 0.007631\n",
            "Epoch 486: Train MSE = 0.007523\n",
            "Epoch 487: Train MSE = 0.007455\n",
            "Epoch 488: Train MSE = 0.007372\n",
            "Epoch 489: Train MSE = 0.007244\n",
            "Epoch 490: Train MSE = 0.007163\n",
            "Epoch 491: Train MSE = 0.007057\n",
            "Epoch 492: Train MSE = 0.006962\n",
            "Epoch 493: Train MSE = 0.006908\n",
            "Epoch 494: Train MSE = 0.006821\n",
            "Epoch 495: Train MSE = 0.006735\n",
            "Epoch 496: Train MSE = 0.006635\n",
            "Epoch 497: Train MSE = 0.006554\n",
            "Epoch 498: Train MSE = 0.006473\n",
            "Epoch 499: Train MSE = 0.006377\n",
            "Epoch 500: Train MSE = 0.006308\n",
            "Epoch 501: Train MSE = 0.006235\n",
            "Epoch 502: Train MSE = 0.006160\n",
            "Epoch 503: Train MSE = 0.006059\n",
            "Epoch 504: Train MSE = 0.005989\n",
            "Epoch 505: Train MSE = 0.005935\n",
            "Epoch 506: Train MSE = 0.005840\n",
            "Epoch 507: Train MSE = 0.005794\n",
            "Epoch 508: Train MSE = 0.005707\n",
            "Epoch 509: Train MSE = 0.005631\n",
            "Epoch 510: Train MSE = 0.005557\n",
            "Epoch 511: Train MSE = 0.005475\n",
            "Epoch 512: Train MSE = 0.005426\n",
            "Epoch 513: Train MSE = 0.005356\n",
            "Epoch 514: Train MSE = 0.005290\n",
            "Epoch 515: Train MSE = 0.005218\n",
            "Epoch 516: Train MSE = 0.005164\n",
            "Epoch 517: Train MSE = 0.005084\n",
            "Epoch 518: Train MSE = 0.005021\n",
            "Epoch 519: Train MSE = 0.004969\n",
            "Epoch 520: Train MSE = 0.004896\n",
            "Epoch 521: Train MSE = 0.004841\n",
            "Epoch 522: Train MSE = 0.004785\n",
            "Epoch 523: Train MSE = 0.004740\n",
            "Epoch 524: Train MSE = 0.004663\n",
            "Epoch 525: Train MSE = 0.004615\n",
            "Epoch 526: Train MSE = 0.004553\n",
            "Epoch 527: Train MSE = 0.004506\n",
            "Epoch 528: Train MSE = 0.004439\n",
            "Epoch 529: Train MSE = 0.004387\n",
            "Epoch 530: Train MSE = 0.004318\n",
            "Epoch 531: Train MSE = 0.004276\n",
            "Epoch 532: Train MSE = 0.004209\n",
            "Epoch 533: Train MSE = 0.004152\n",
            "Epoch 534: Train MSE = 0.004113\n",
            "Epoch 535: Train MSE = 0.004049\n",
            "Epoch 536: Train MSE = 0.003996\n",
            "Epoch 537: Train MSE = 0.003966\n",
            "Epoch 538: Train MSE = 0.003919\n",
            "Epoch 539: Train MSE = 0.003866\n",
            "Epoch 540: Train MSE = 0.003818\n",
            "Epoch 541: Train MSE = 0.003771\n",
            "Epoch 542: Train MSE = 0.003720\n",
            "Epoch 543: Train MSE = 0.003681\n",
            "Epoch 544: Train MSE = 0.003635\n",
            "Epoch 545: Train MSE = 0.003584\n",
            "Epoch 546: Train MSE = 0.003538\n",
            "Epoch 547: Train MSE = 0.003485\n",
            "Epoch 548: Train MSE = 0.003463\n",
            "Epoch 549: Train MSE = 0.003404\n",
            "Epoch 550: Train MSE = 0.003372\n",
            "Epoch 551: Train MSE = 0.003336\n",
            "Epoch 552: Train MSE = 0.003286\n",
            "Epoch 553: Train MSE = 0.003237\n",
            "Epoch 554: Train MSE = 0.003204\n",
            "Epoch 555: Train MSE = 0.003158\n",
            "Epoch 556: Train MSE = 0.003126\n",
            "Epoch 557: Train MSE = 0.003080\n",
            "Epoch 558: Train MSE = 0.003047\n",
            "Epoch 559: Train MSE = 0.003006\n",
            "Epoch 560: Train MSE = 0.002971\n",
            "Epoch 561: Train MSE = 0.002938\n",
            "Epoch 562: Train MSE = 0.002901\n",
            "Epoch 563: Train MSE = 0.002868\n",
            "Epoch 564: Train MSE = 0.002822\n",
            "Epoch 565: Train MSE = 0.002799\n",
            "Epoch 566: Train MSE = 0.002759\n",
            "Epoch 567: Train MSE = 0.002722\n",
            "Epoch 568: Train MSE = 0.002701\n",
            "Epoch 569: Train MSE = 0.002652\n",
            "Epoch 570: Train MSE = 0.002631\n",
            "Epoch 571: Train MSE = 0.002588\n",
            "Epoch 572: Train MSE = 0.002558\n",
            "Epoch 573: Train MSE = 0.002525\n",
            "Epoch 574: Train MSE = 0.002501\n",
            "Epoch 575: Train MSE = 0.002465\n",
            "Epoch 576: Train MSE = 0.002430\n",
            "Epoch 577: Train MSE = 0.002410\n",
            "Epoch 578: Train MSE = 0.002375\n",
            "Epoch 579: Train MSE = 0.002349\n",
            "Epoch 580: Train MSE = 0.002317\n",
            "Epoch 581: Train MSE = 0.002293\n",
            "Epoch 582: Train MSE = 0.002257\n",
            "Epoch 583: Train MSE = 0.002228\n",
            "Epoch 584: Train MSE = 0.002199\n",
            "Epoch 585: Train MSE = 0.002177\n",
            "Epoch 586: Train MSE = 0.002149\n",
            "Epoch 587: Train MSE = 0.002133\n",
            "Epoch 588: Train MSE = 0.002097\n",
            "Epoch 589: Train MSE = 0.002076\n",
            "Epoch 590: Train MSE = 0.002045\n",
            "Epoch 591: Train MSE = 0.002021\n",
            "Epoch 592: Train MSE = 0.002003\n",
            "Epoch 593: Train MSE = 0.001982\n",
            "Epoch 594: Train MSE = 0.001945\n",
            "Epoch 595: Train MSE = 0.001924\n",
            "Epoch 596: Train MSE = 0.001907\n",
            "Epoch 597: Train MSE = 0.001876\n",
            "Epoch 598: Train MSE = 0.001861\n",
            "Epoch 599: Train MSE = 0.001836\n",
            "Epoch 600: Train MSE = 0.001809\n",
            "Epoch 601: Train MSE = 0.001791\n",
            "Epoch 602: Train MSE = 0.001768\n",
            "Epoch 603: Train MSE = 0.001748\n",
            "Epoch 604: Train MSE = 0.001724\n",
            "Epoch 605: Train MSE = 0.001707\n",
            "Epoch 606: Train MSE = 0.001679\n",
            "Epoch 607: Train MSE = 0.001661\n",
            "Epoch 608: Train MSE = 0.001640\n",
            "Epoch 609: Train MSE = 0.001616\n",
            "Epoch 610: Train MSE = 0.001598\n",
            "Epoch 611: Train MSE = 0.001583\n",
            "Epoch 612: Train MSE = 0.001560\n",
            "Epoch 613: Train MSE = 0.001542\n",
            "Epoch 614: Train MSE = 0.001524\n",
            "Epoch 615: Train MSE = 0.001503\n",
            "Epoch 616: Train MSE = 0.001485\n",
            "Epoch 617: Train MSE = 0.001469\n",
            "Epoch 618: Train MSE = 0.001452\n",
            "Epoch 619: Train MSE = 0.001436\n",
            "Epoch 620: Train MSE = 0.001417\n",
            "Epoch 621: Train MSE = 0.001398\n",
            "Epoch 622: Train MSE = 0.001380\n",
            "Epoch 623: Train MSE = 0.001365\n",
            "Epoch 624: Train MSE = 0.001348\n",
            "Epoch 625: Train MSE = 0.001333\n",
            "Epoch 626: Train MSE = 0.001319\n",
            "Epoch 627: Train MSE = 0.001298\n",
            "Epoch 628: Train MSE = 0.001284\n",
            "Epoch 629: Train MSE = 0.001269\n",
            "Epoch 630: Train MSE = 0.001252\n",
            "Epoch 631: Train MSE = 0.001237\n",
            "Epoch 632: Train MSE = 0.001222\n",
            "Epoch 633: Train MSE = 0.001201\n",
            "Epoch 634: Train MSE = 0.001193\n",
            "Epoch 635: Train MSE = 0.001179\n",
            "Epoch 636: Train MSE = 0.001165\n",
            "Epoch 637: Train MSE = 0.001149\n",
            "Epoch 638: Train MSE = 0.001136\n",
            "Epoch 639: Train MSE = 0.001120\n",
            "Epoch 640: Train MSE = 0.001109\n",
            "Epoch 641: Train MSE = 0.001094\n",
            "Epoch 642: Train MSE = 0.001082\n",
            "Epoch 643: Train MSE = 0.001069\n",
            "Epoch 644: Train MSE = 0.001057\n",
            "Epoch 645: Train MSE = 0.001045\n",
            "Epoch 646: Train MSE = 0.001030\n",
            "Epoch 647: Train MSE = 0.001017\n",
            "Epoch 648: Train MSE = 0.001008\n",
            "Epoch 649: Train MSE = 0.000996\n",
            "Epoch 650: Train MSE = 0.000982\n",
            "Epoch 651: Train MSE = 0.000968\n",
            "Epoch 652: Train MSE = 0.000958\n",
            "Epoch 653: Train MSE = 0.000944\n",
            "Epoch 654: Train MSE = 0.000933\n",
            "Epoch 655: Train MSE = 0.000925\n",
            "Epoch 656: Train MSE = 0.000913\n",
            "Epoch 657: Train MSE = 0.000900\n",
            "Epoch 658: Train MSE = 0.000892\n",
            "Epoch 659: Train MSE = 0.000880\n",
            "Epoch 660: Train MSE = 0.000867\n",
            "Epoch 661: Train MSE = 0.000857\n",
            "Epoch 662: Train MSE = 0.000850\n",
            "Epoch 663: Train MSE = 0.000835\n",
            "Epoch 664: Train MSE = 0.000828\n",
            "Epoch 665: Train MSE = 0.000819\n",
            "Epoch 666: Train MSE = 0.000808\n",
            "Epoch 667: Train MSE = 0.000797\n",
            "Epoch 668: Train MSE = 0.000791\n",
            "Epoch 669: Train MSE = 0.000779\n",
            "Epoch 670: Train MSE = 0.000769\n",
            "Epoch 671: Train MSE = 0.000759\n",
            "Epoch 672: Train MSE = 0.000751\n",
            "Epoch 673: Train MSE = 0.000744\n",
            "Epoch 674: Train MSE = 0.000733\n",
            "Epoch 675: Train MSE = 0.000723\n",
            "Epoch 676: Train MSE = 0.000717\n",
            "Epoch 677: Train MSE = 0.000709\n",
            "Epoch 678: Train MSE = 0.000699\n",
            "Epoch 679: Train MSE = 0.000690\n",
            "Epoch 680: Train MSE = 0.000681\n",
            "Epoch 681: Train MSE = 0.000673\n",
            "Epoch 682: Train MSE = 0.000663\n",
            "Epoch 683: Train MSE = 0.000658\n",
            "Epoch 684: Train MSE = 0.000649\n",
            "Epoch 685: Train MSE = 0.000643\n",
            "Epoch 686: Train MSE = 0.000634\n",
            "Epoch 687: Train MSE = 0.000627\n",
            "Epoch 688: Train MSE = 0.000619\n",
            "Epoch 689: Train MSE = 0.000611\n",
            "Epoch 690: Train MSE = 0.000603\n",
            "Epoch 691: Train MSE = 0.000597\n",
            "Epoch 692: Train MSE = 0.000591\n",
            "Epoch 693: Train MSE = 0.000583\n",
            "Epoch 694: Train MSE = 0.000574\n",
            "Epoch 695: Train MSE = 0.000570\n",
            "Epoch 696: Train MSE = 0.000562\n",
            "Epoch 697: Train MSE = 0.000556\n",
            "Epoch 698: Train MSE = 0.000550\n",
            "Epoch 699: Train MSE = 0.000542\n",
            "Epoch 700: Train MSE = 0.000534\n",
            "Epoch 701: Train MSE = 0.000529\n",
            "Epoch 702: Train MSE = 0.000523\n",
            "Epoch 703: Train MSE = 0.000516\n",
            "Epoch 704: Train MSE = 0.000510\n",
            "Epoch 705: Train MSE = 0.000505\n",
            "Epoch 706: Train MSE = 0.000498\n",
            "Epoch 707: Train MSE = 0.000492\n",
            "Epoch 708: Train MSE = 0.000484\n",
            "Epoch 709: Train MSE = 0.000479\n",
            "Epoch 710: Train MSE = 0.000474\n",
            "Epoch 711: Train MSE = 0.000469\n",
            "Epoch 712: Train MSE = 0.000464\n",
            "Epoch 713: Train MSE = 0.000457\n",
            "Epoch 714: Train MSE = 0.000453\n",
            "Epoch 715: Train MSE = 0.000447\n",
            "Epoch 716: Train MSE = 0.000442\n",
            "Epoch 717: Train MSE = 0.000435\n",
            "Epoch 718: Train MSE = 0.000432\n",
            "Epoch 719: Train MSE = 0.000427\n",
            "Epoch 720: Train MSE = 0.000422\n",
            "Epoch 721: Train MSE = 0.000414\n",
            "Epoch 722: Train MSE = 0.000411\n",
            "Epoch 723: Train MSE = 0.000406\n",
            "Epoch 724: Train MSE = 0.000402\n",
            "Epoch 725: Train MSE = 0.000396\n",
            "Epoch 726: Train MSE = 0.000392\n",
            "Epoch 727: Train MSE = 0.000386\n",
            "Epoch 728: Train MSE = 0.000382\n",
            "Epoch 729: Train MSE = 0.000379\n",
            "Epoch 730: Train MSE = 0.000374\n",
            "Epoch 731: Train MSE = 0.000369\n",
            "Epoch 732: Train MSE = 0.000364\n",
            "Epoch 733: Train MSE = 0.000360\n",
            "Epoch 734: Train MSE = 0.000356\n",
            "Epoch 735: Train MSE = 0.000351\n",
            "Epoch 736: Train MSE = 0.000347\n",
            "Epoch 737: Train MSE = 0.000344\n",
            "Epoch 738: Train MSE = 0.000339\n",
            "Epoch 739: Train MSE = 0.000335\n",
            "Epoch 740: Train MSE = 0.000331\n",
            "Epoch 741: Train MSE = 0.000328\n",
            "Epoch 742: Train MSE = 0.000323\n",
            "Epoch 743: Train MSE = 0.000319\n",
            "Epoch 744: Train MSE = 0.000315\n",
            "Epoch 745: Train MSE = 0.000312\n",
            "Epoch 746: Train MSE = 0.000309\n",
            "Epoch 747: Train MSE = 0.000305\n",
            "Epoch 748: Train MSE = 0.000301\n",
            "Epoch 749: Train MSE = 0.000298\n",
            "Epoch 750: Train MSE = 0.000294\n",
            "Epoch 751: Train MSE = 0.000289\n",
            "Epoch 752: Train MSE = 0.000286\n",
            "Epoch 753: Train MSE = 0.000284\n",
            "Epoch 754: Train MSE = 0.000280\n",
            "Epoch 755: Train MSE = 0.000276\n",
            "Epoch 756: Train MSE = 0.000273\n",
            "Epoch 757: Train MSE = 0.000270\n",
            "Epoch 758: Train MSE = 0.000267\n",
            "Epoch 759: Train MSE = 0.000264\n",
            "Epoch 760: Train MSE = 0.000261\n",
            "Epoch 761: Train MSE = 0.000258\n",
            "Epoch 762: Train MSE = 0.000254\n",
            "Epoch 763: Train MSE = 0.000251\n",
            "Epoch 764: Train MSE = 0.000248\n",
            "Epoch 765: Train MSE = 0.000245\n",
            "Epoch 766: Train MSE = 0.000243\n",
            "Epoch 767: Train MSE = 0.000239\n",
            "Epoch 768: Train MSE = 0.000237\n",
            "Epoch 769: Train MSE = 0.000234\n",
            "Epoch 770: Train MSE = 0.000231\n",
            "Epoch 771: Train MSE = 0.000228\n",
            "Epoch 772: Train MSE = 0.000226\n",
            "Epoch 773: Train MSE = 0.000223\n",
            "Epoch 774: Train MSE = 0.000220\n",
            "Epoch 775: Train MSE = 0.000218\n",
            "Epoch 776: Train MSE = 0.000216\n",
            "Epoch 777: Train MSE = 0.000212\n",
            "Epoch 778: Train MSE = 0.000210\n",
            "Epoch 779: Train MSE = 0.000208\n",
            "Epoch 780: Train MSE = 0.000205\n",
            "Epoch 781: Train MSE = 0.000203\n",
            "Epoch 782: Train MSE = 0.000200\n",
            "Epoch 783: Train MSE = 0.000198\n",
            "Epoch 784: Train MSE = 0.000197\n",
            "Epoch 785: Train MSE = 0.000193\n",
            "Epoch 786: Train MSE = 0.000191\n",
            "Epoch 787: Train MSE = 0.000189\n",
            "Epoch 788: Train MSE = 0.000186\n",
            "Epoch 789: Train MSE = 0.000185\n",
            "Epoch 790: Train MSE = 0.000182\n",
            "Epoch 791: Train MSE = 0.000180\n",
            "Epoch 792: Train MSE = 0.000178\n",
            "Epoch 793: Train MSE = 0.000176\n",
            "Epoch 794: Train MSE = 0.000174\n",
            "Epoch 795: Train MSE = 0.000172\n",
            "Epoch 796: Train MSE = 0.000170\n",
            "Epoch 797: Train MSE = 0.000168\n",
            "Epoch 798: Train MSE = 0.000166\n",
            "Epoch 799: Train MSE = 0.000164\n",
            "Epoch 800: Train MSE = 0.000162\n",
            "Epoch 801: Train MSE = 0.000161\n",
            "Epoch 802: Train MSE = 0.000158\n",
            "Epoch 803: Train MSE = 0.000156\n",
            "Epoch 804: Train MSE = 0.000154\n",
            "Epoch 805: Train MSE = 0.000153\n",
            "Epoch 806: Train MSE = 0.000151\n",
            "Epoch 807: Train MSE = 0.000149\n",
            "Epoch 808: Train MSE = 0.000147\n",
            "Epoch 809: Train MSE = 0.000145\n",
            "Epoch 810: Train MSE = 0.000144\n",
            "Epoch 811: Train MSE = 0.000142\n",
            "Epoch 812: Train MSE = 0.000140\n",
            "Epoch 813: Train MSE = 0.000139\n",
            "Epoch 814: Train MSE = 0.000137\n",
            "Epoch 815: Train MSE = 0.000135\n",
            "Epoch 816: Train MSE = 0.000134\n",
            "Epoch 817: Train MSE = 0.000133\n",
            "Epoch 818: Train MSE = 0.000131\n",
            "Epoch 819: Train MSE = 0.000129\n",
            "Epoch 820: Train MSE = 0.000128\n",
            "Epoch 821: Train MSE = 0.000126\n",
            "Epoch 822: Train MSE = 0.000125\n",
            "Epoch 823: Train MSE = 0.000123\n",
            "Epoch 824: Train MSE = 0.000122\n",
            "Epoch 825: Train MSE = 0.000121\n",
            "Epoch 826: Train MSE = 0.000119\n",
            "Epoch 827: Train MSE = 0.000118\n",
            "Epoch 828: Train MSE = 0.000116\n",
            "Epoch 829: Train MSE = 0.000115\n",
            "Epoch 830: Train MSE = 0.000114\n",
            "Epoch 831: Train MSE = 0.000112\n",
            "Epoch 832: Train MSE = 0.000111\n",
            "Epoch 833: Train MSE = 0.000110\n",
            "Epoch 834: Train MSE = 0.000108\n",
            "Epoch 835: Train MSE = 0.000107\n",
            "Epoch 836: Train MSE = 0.000106\n",
            "Epoch 837: Train MSE = 0.000105\n",
            "Epoch 838: Train MSE = 0.000103\n",
            "Epoch 839: Train MSE = 0.000102\n",
            "Epoch 840: Train MSE = 0.000101\n",
            "Epoch 841: Train MSE = 0.000100\n",
            "Epoch 842: Train MSE = 0.000098\n",
            "Epoch 843: Train MSE = 0.000098\n",
            "Epoch 844: Train MSE = 0.000096\n",
            "Epoch 845: Train MSE = 0.000095\n",
            "Epoch 846: Train MSE = 0.000094\n",
            "Epoch 847: Train MSE = 0.000093\n",
            "Epoch 848: Train MSE = 0.000092\n",
            "Epoch 849: Train MSE = 0.000091\n",
            "Epoch 850: Train MSE = 0.000090\n",
            "Epoch 851: Train MSE = 0.000088\n",
            "Epoch 852: Train MSE = 0.000088\n",
            "Epoch 853: Train MSE = 0.000086\n",
            "Epoch 854: Train MSE = 0.000086\n",
            "Epoch 855: Train MSE = 0.000085\n",
            "Epoch 856: Train MSE = 0.000084\n",
            "Epoch 857: Train MSE = 0.000083\n",
            "Epoch 858: Train MSE = 0.000081\n",
            "Epoch 859: Train MSE = 0.000081\n",
            "Epoch 860: Train MSE = 0.000080\n",
            "Epoch 861: Train MSE = 0.000079\n",
            "Epoch 862: Train MSE = 0.000078\n",
            "Epoch 863: Train MSE = 0.000077\n",
            "Epoch 864: Train MSE = 0.000076\n",
            "Epoch 865: Train MSE = 0.000075\n",
            "Epoch 866: Train MSE = 0.000074\n",
            "Epoch 867: Train MSE = 0.000073\n",
            "Epoch 868: Train MSE = 0.000073\n",
            "Epoch 869: Train MSE = 0.000072\n",
            "Epoch 870: Train MSE = 0.000071\n",
            "Epoch 871: Train MSE = 0.000070\n",
            "Epoch 872: Train MSE = 0.000069\n",
            "Epoch 873: Train MSE = 0.000069\n",
            "Epoch 874: Train MSE = 0.000068\n",
            "Epoch 875: Train MSE = 0.000067\n",
            "Epoch 876: Train MSE = 0.000066\n",
            "Epoch 877: Train MSE = 0.000065\n",
            "Epoch 878: Train MSE = 0.000064\n",
            "Epoch 879: Train MSE = 0.000064\n",
            "Epoch 880: Train MSE = 0.000063\n",
            "Epoch 881: Train MSE = 0.000062\n",
            "Epoch 882: Train MSE = 0.000062\n",
            "Epoch 883: Train MSE = 0.000061\n",
            "Epoch 884: Train MSE = 0.000060\n",
            "Epoch 885: Train MSE = 0.000059\n",
            "Epoch 886: Train MSE = 0.000059\n",
            "Epoch 887: Train MSE = 0.000058\n",
            "Epoch 888: Train MSE = 0.000058\n",
            "Epoch 889: Train MSE = 0.000057\n",
            "Epoch 890: Train MSE = 0.000056\n",
            "Epoch 891: Train MSE = 0.000055\n",
            "Epoch 892: Train MSE = 0.000055\n",
            "Epoch 893: Train MSE = 0.000054\n",
            "Epoch 894: Train MSE = 0.000053\n",
            "Epoch 895: Train MSE = 0.000053\n",
            "Epoch 896: Train MSE = 0.000052\n",
            "Epoch 897: Train MSE = 0.000052\n",
            "Epoch 898: Train MSE = 0.000051\n",
            "Epoch 899: Train MSE = 0.000050\n",
            "Epoch 900: Train MSE = 0.000050\n",
            "Epoch 901: Train MSE = 0.000049\n",
            "Epoch 902: Train MSE = 0.000049\n",
            "Epoch 903: Train MSE = 0.000048\n",
            "Epoch 904: Train MSE = 0.000048\n",
            "Epoch 905: Train MSE = 0.000047\n",
            "Epoch 906: Train MSE = 0.000047\n",
            "Epoch 907: Train MSE = 0.000046\n",
            "Epoch 908: Train MSE = 0.000045\n",
            "Epoch 909: Train MSE = 0.000045\n",
            "Epoch 910: Train MSE = 0.000044\n",
            "Epoch 911: Train MSE = 0.000044\n",
            "Epoch 912: Train MSE = 0.000043\n",
            "Epoch 913: Train MSE = 0.000043\n",
            "Epoch 914: Train MSE = 0.000042\n",
            "Epoch 915: Train MSE = 0.000042\n",
            "Epoch 916: Train MSE = 0.000041\n",
            "Epoch 917: Train MSE = 0.000041\n",
            "Epoch 918: Train MSE = 0.000040\n",
            "Epoch 919: Train MSE = 0.000040\n",
            "Epoch 920: Train MSE = 0.000039\n",
            "Epoch 921: Train MSE = 0.000039\n",
            "Epoch 922: Train MSE = 0.000039\n",
            "Epoch 923: Train MSE = 0.000038\n",
            "Epoch 924: Train MSE = 0.000038\n",
            "Epoch 925: Train MSE = 0.000037\n",
            "Epoch 926: Train MSE = 0.000037\n",
            "Epoch 927: Train MSE = 0.000036\n",
            "Epoch 928: Train MSE = 0.000036\n",
            "Epoch 929: Train MSE = 0.000036\n",
            "Epoch 930: Train MSE = 0.000035\n",
            "Epoch 931: Train MSE = 0.000035\n",
            "Epoch 932: Train MSE = 0.000034\n",
            "Epoch 933: Train MSE = 0.000034\n",
            "Epoch 934: Train MSE = 0.000034\n",
            "Epoch 935: Train MSE = 0.000033\n",
            "Epoch 936: Train MSE = 0.000033\n",
            "Epoch 937: Train MSE = 0.000032\n",
            "Epoch 938: Train MSE = 0.000032\n",
            "Epoch 939: Train MSE = 0.000032\n",
            "Epoch 940: Train MSE = 0.000031\n",
            "Epoch 941: Train MSE = 0.000031\n",
            "Epoch 942: Train MSE = 0.000031\n",
            "Epoch 943: Train MSE = 0.000030\n",
            "Epoch 944: Train MSE = 0.000030\n",
            "Epoch 945: Train MSE = 0.000030\n",
            "Epoch 946: Train MSE = 0.000029\n",
            "Epoch 947: Train MSE = 0.000029\n",
            "Epoch 948: Train MSE = 0.000028\n",
            "Epoch 949: Train MSE = 0.000028\n",
            "Epoch 950: Train MSE = 0.000028\n",
            "Epoch 951: Train MSE = 0.000027\n",
            "Epoch 952: Train MSE = 0.000027\n",
            "Epoch 953: Train MSE = 0.000027\n",
            "Epoch 954: Train MSE = 0.000027\n",
            "Epoch 955: Train MSE = 0.000026\n",
            "Epoch 956: Train MSE = 0.000026\n",
            "Epoch 957: Train MSE = 0.000026\n",
            "Epoch 958: Train MSE = 0.000025\n",
            "Epoch 959: Train MSE = 0.000025\n",
            "Epoch 960: Train MSE = 0.000025\n",
            "Epoch 961: Train MSE = 0.000024\n",
            "Epoch 962: Train MSE = 0.000024\n",
            "Epoch 963: Train MSE = 0.000024\n",
            "Epoch 964: Train MSE = 0.000024\n",
            "Epoch 965: Train MSE = 0.000023\n",
            "Epoch 966: Train MSE = 0.000023\n",
            "Epoch 967: Train MSE = 0.000023\n",
            "Epoch 968: Train MSE = 0.000023\n",
            "Epoch 969: Train MSE = 0.000022\n",
            "Epoch 970: Train MSE = 0.000022\n",
            "Epoch 971: Train MSE = 0.000022\n",
            "Epoch 972: Train MSE = 0.000022\n",
            "Epoch 973: Train MSE = 0.000021\n",
            "Epoch 974: Train MSE = 0.000021\n",
            "Epoch 975: Train MSE = 0.000021\n",
            "Epoch 976: Train MSE = 0.000021\n",
            "Epoch 977: Train MSE = 0.000020\n",
            "Epoch 978: Train MSE = 0.000020\n",
            "Epoch 979: Train MSE = 0.000020\n",
            "Epoch 980: Train MSE = 0.000020\n",
            "Epoch 981: Train MSE = 0.000019\n",
            "Epoch 982: Train MSE = 0.000019\n",
            "Epoch 983: Train MSE = 0.000019\n",
            "Epoch 984: Train MSE = 0.000019\n",
            "Epoch 985: Train MSE = 0.000019\n",
            "Epoch 986: Train MSE = 0.000018\n",
            "Epoch 987: Train MSE = 0.000018\n",
            "Epoch 988: Train MSE = 0.000018\n",
            "Epoch 989: Train MSE = 0.000018\n",
            "Epoch 990: Train MSE = 0.000018\n",
            "Epoch 991: Train MSE = 0.000017\n",
            "Epoch 992: Train MSE = 0.000017\n",
            "Epoch 993: Train MSE = 0.000017\n",
            "Epoch 994: Train MSE = 0.000017\n",
            "Epoch 995: Train MSE = 0.000017\n",
            "Epoch 996: Train MSE = 0.000016\n",
            "Epoch 997: Train MSE = 0.000016\n",
            "Epoch 998: Train MSE = 0.000016\n",
            "Epoch 999: Train MSE = 0.000016\n",
            "Epoch 1000: Train MSE = 0.000016\n",
            "Epoch 1001: Train MSE = 0.000015\n",
            "Epoch 1002: Train MSE = 0.000015\n",
            "Epoch 1003: Train MSE = 0.000015\n",
            "Epoch 1004: Train MSE = 0.000015\n",
            "Epoch 1005: Train MSE = 0.000015\n",
            "Epoch 1006: Train MSE = 0.000015\n",
            "Epoch 1007: Train MSE = 0.000014\n",
            "Epoch 1008: Train MSE = 0.000014\n",
            "Epoch 1009: Train MSE = 0.000014\n",
            "Epoch 1010: Train MSE = 0.000014\n",
            "Epoch 1011: Train MSE = 0.000014\n",
            "Epoch 1012: Train MSE = 0.000014\n",
            "Epoch 1013: Train MSE = 0.000013\n",
            "Epoch 1014: Train MSE = 0.000013\n",
            "Epoch 1015: Train MSE = 0.000013\n",
            "Epoch 1016: Train MSE = 0.000013\n",
            "Epoch 1017: Train MSE = 0.000013\n",
            "Epoch 1018: Train MSE = 0.000013\n",
            "Epoch 1019: Train MSE = 0.000013\n",
            "Epoch 1020: Train MSE = 0.000012\n",
            "Epoch 1021: Train MSE = 0.000012\n",
            "Epoch 1022: Train MSE = 0.000012\n",
            "Epoch 1023: Train MSE = 0.000012\n",
            "Epoch 1024: Train MSE = 0.000012\n",
            "Epoch 1025: Train MSE = 0.000012\n",
            "Epoch 1026: Train MSE = 0.000012\n",
            "Epoch 1027: Train MSE = 0.000011\n",
            "Epoch 1028: Train MSE = 0.000011\n",
            "Epoch 1029: Train MSE = 0.000011\n",
            "Epoch 1030: Train MSE = 0.000011\n",
            "Epoch 1031: Train MSE = 0.000011\n",
            "Epoch 1032: Train MSE = 0.000011\n",
            "Epoch 1033: Train MSE = 0.000011\n",
            "Epoch 1034: Train MSE = 0.000011\n",
            "Epoch 1035: Train MSE = 0.000010\n",
            "Epoch 1036: Train MSE = 0.000010\n",
            "Epoch 1037: Train MSE = 0.000010\n",
            "Epoch 1038: Train MSE = 0.000010\n",
            "Epoch 1039: Train MSE = 0.000010\n",
            "Epoch 1040: Train MSE = 0.000010\n",
            "Epoch 1041: Train MSE = 0.000010\n",
            "Epoch 1042: Train MSE = 0.000010\n",
            "Epoch 1043: Train MSE = 0.000010\n",
            "Epoch 1044: Train MSE = 0.000009\n",
            "Epoch 1045: Train MSE = 0.000009\n",
            "Epoch 1046: Train MSE = 0.000009\n",
            "Epoch 1047: Train MSE = 0.000009\n",
            "Epoch 1048: Train MSE = 0.000009\n",
            "Epoch 1049: Train MSE = 0.000009\n",
            "Epoch 1050: Train MSE = 0.000009\n",
            "Epoch 1051: Train MSE = 0.000009\n",
            "Epoch 1052: Train MSE = 0.000009\n",
            "Epoch 1053: Train MSE = 0.000008\n",
            "Epoch 1054: Train MSE = 0.000008\n",
            "Epoch 1055: Train MSE = 0.000008\n",
            "Epoch 1056: Train MSE = 0.000008\n",
            "Epoch 1057: Train MSE = 0.000008\n",
            "Epoch 1058: Train MSE = 0.000008\n",
            "Epoch 1059: Train MSE = 0.000008\n",
            "Epoch 1060: Train MSE = 0.000008\n",
            "Epoch 1061: Train MSE = 0.000008\n",
            "Epoch 1062: Train MSE = 0.000008\n",
            "Epoch 1063: Train MSE = 0.000008\n",
            "Epoch 1064: Train MSE = 0.000007\n",
            "Epoch 1065: Train MSE = 0.000007\n",
            "Epoch 1066: Train MSE = 0.000007\n",
            "Epoch 1067: Train MSE = 0.000007\n",
            "Epoch 1068: Train MSE = 0.000007\n",
            "Epoch 1069: Train MSE = 0.000007\n",
            "Epoch 1070: Train MSE = 0.000007\n",
            "Epoch 1071: Train MSE = 0.000007\n",
            "Epoch 1072: Train MSE = 0.000007\n",
            "Epoch 1073: Train MSE = 0.000007\n",
            "Epoch 1074: Train MSE = 0.000007\n",
            "Epoch 1075: Train MSE = 0.000007\n",
            "Epoch 1076: Train MSE = 0.000007\n",
            "Epoch 1077: Train MSE = 0.000006\n",
            "Epoch 1078: Train MSE = 0.000006\n",
            "Epoch 1079: Train MSE = 0.000006\n",
            "Epoch 1080: Train MSE = 0.000006\n",
            "Epoch 1081: Train MSE = 0.000006\n",
            "Epoch 1082: Train MSE = 0.000006\n",
            "Epoch 1083: Train MSE = 0.000006\n",
            "Epoch 1084: Train MSE = 0.000006\n",
            "Epoch 1085: Train MSE = 0.000006\n",
            "Epoch 1086: Train MSE = 0.000006\n",
            "Epoch 1087: Train MSE = 0.000006\n",
            "Epoch 1088: Train MSE = 0.000006\n",
            "Epoch 1089: Train MSE = 0.000006\n",
            "Epoch 1090: Train MSE = 0.000006\n",
            "Epoch 1091: Train MSE = 0.000005\n",
            "Epoch 1092: Train MSE = 0.000005\n",
            "Epoch 1093: Train MSE = 0.000005\n",
            "Epoch 1094: Train MSE = 0.000005\n",
            "Epoch 1095: Train MSE = 0.000005\n",
            "Epoch 1096: Train MSE = 0.000005\n",
            "Epoch 1097: Train MSE = 0.000005\n",
            "Epoch 1098: Train MSE = 0.000005\n",
            "Epoch 1099: Train MSE = 0.000005\n",
            "Epoch 1100: Train MSE = 0.000005\n",
            "Epoch 1101: Train MSE = 0.000005\n",
            "Epoch 1102: Train MSE = 0.000005\n",
            "Epoch 1103: Train MSE = 0.000005\n",
            "Epoch 1104: Train MSE = 0.000005\n",
            "Epoch 1105: Train MSE = 0.000005\n",
            "Epoch 1106: Train MSE = 0.000005\n",
            "Epoch 1107: Train MSE = 0.000005\n",
            "Epoch 1108: Train MSE = 0.000005\n",
            "Epoch 1109: Train MSE = 0.000004\n",
            "Epoch 1110: Train MSE = 0.000004\n",
            "Epoch 1111: Train MSE = 0.000004\n",
            "Epoch 1112: Train MSE = 0.000004\n",
            "Epoch 1113: Train MSE = 0.000004\n",
            "Epoch 1114: Train MSE = 0.000004\n",
            "Epoch 1115: Train MSE = 0.000004\n",
            "Epoch 1116: Train MSE = 0.000004\n",
            "Epoch 1117: Train MSE = 0.000004\n",
            "Epoch 1118: Train MSE = 0.000004\n",
            "Epoch 1119: Train MSE = 0.000004\n",
            "Epoch 1120: Train MSE = 0.000004\n",
            "Epoch 1121: Train MSE = 0.000004\n",
            "Epoch 1122: Train MSE = 0.000004\n",
            "Epoch 1123: Train MSE = 0.000004\n",
            "Epoch 1124: Train MSE = 0.000004\n",
            "Epoch 1125: Train MSE = 0.000004\n",
            "Epoch 1126: Train MSE = 0.000004\n",
            "Epoch 1127: Train MSE = 0.000004\n",
            "Epoch 1128: Train MSE = 0.000004\n",
            "Epoch 1129: Train MSE = 0.000004\n",
            "Epoch 1130: Train MSE = 0.000004\n",
            "Epoch 1131: Train MSE = 0.000003\n",
            "Epoch 1132: Train MSE = 0.000003\n",
            "Epoch 1133: Train MSE = 0.000003\n",
            "Epoch 1134: Train MSE = 0.000003\n",
            "Epoch 1135: Train MSE = 0.000003\n",
            "Epoch 1136: Train MSE = 0.000003\n",
            "Epoch 1137: Train MSE = 0.000003\n",
            "Epoch 1138: Train MSE = 0.000003\n",
            "Epoch 1139: Train MSE = 0.000003\n",
            "Epoch 1140: Train MSE = 0.000003\n",
            "Epoch 1141: Train MSE = 0.000003\n",
            "Epoch 1142: Train MSE = 0.000003\n",
            "Epoch 1143: Train MSE = 0.000003\n",
            "Epoch 1144: Train MSE = 0.000003\n",
            "Epoch 1145: Train MSE = 0.000003\n",
            "Epoch 1146: Train MSE = 0.000003\n",
            "Epoch 1147: Train MSE = 0.000003\n",
            "Epoch 1148: Train MSE = 0.000003\n",
            "Epoch 1149: Train MSE = 0.000003\n",
            "Epoch 1150: Train MSE = 0.000003\n",
            "Epoch 1151: Train MSE = 0.000003\n",
            "Epoch 1152: Train MSE = 0.000003\n",
            "Epoch 1153: Train MSE = 0.000003\n",
            "Epoch 1154: Train MSE = 0.000003\n",
            "Epoch 1155: Train MSE = 0.000003\n",
            "Epoch 1156: Train MSE = 0.000003\n",
            "Epoch 1157: Train MSE = 0.000003\n",
            "Epoch 1158: Train MSE = 0.000003\n",
            "Epoch 1159: Train MSE = 0.000003\n",
            "Epoch 1160: Train MSE = 0.000002\n",
            "Epoch 1161: Train MSE = 0.000002\n",
            "Epoch 1162: Train MSE = 0.000002\n",
            "Epoch 1163: Train MSE = 0.000002\n",
            "Epoch 1164: Train MSE = 0.000002\n",
            "Epoch 1165: Train MSE = 0.000002\n",
            "Epoch 1166: Train MSE = 0.000002\n",
            "Epoch 1167: Train MSE = 0.000002\n",
            "Epoch 1168: Train MSE = 0.000002\n",
            "Epoch 1169: Train MSE = 0.000002\n",
            "Epoch 1170: Train MSE = 0.000002\n",
            "Epoch 1171: Train MSE = 0.000002\n",
            "Epoch 1172: Train MSE = 0.000002\n",
            "Epoch 1173: Train MSE = 0.000002\n",
            "Epoch 1174: Train MSE = 0.000002\n",
            "Epoch 1175: Train MSE = 0.000002\n",
            "Epoch 1176: Train MSE = 0.000002\n",
            "Epoch 1177: Train MSE = 0.000002\n",
            "Epoch 1178: Train MSE = 0.000002\n",
            "Epoch 1179: Train MSE = 0.000002\n",
            "Epoch 1180: Train MSE = 0.000002\n",
            "Epoch 1181: Train MSE = 0.000002\n",
            "Epoch 1182: Train MSE = 0.000002\n",
            "Epoch 1183: Train MSE = 0.000002\n",
            "Epoch 1184: Train MSE = 0.000002\n",
            "Epoch 1185: Train MSE = 0.000002\n",
            "Epoch 1186: Train MSE = 0.000002\n",
            "Epoch 1187: Train MSE = 0.000002\n",
            "Epoch 1188: Train MSE = 0.000002\n",
            "Epoch 1189: Train MSE = 0.000002\n",
            "Epoch 1190: Train MSE = 0.000002\n",
            "Epoch 1191: Train MSE = 0.000002\n",
            "Epoch 1192: Train MSE = 0.000002\n",
            "Epoch 1193: Train MSE = 0.000002\n",
            "Epoch 1194: Train MSE = 0.000002\n",
            "Epoch 1195: Train MSE = 0.000002\n",
            "Epoch 1196: Train MSE = 0.000002\n",
            "Epoch 1197: Train MSE = 0.000002\n",
            "Epoch 1198: Train MSE = 0.000002\n",
            "Epoch 1199: Train MSE = 0.000002\n",
            "Epoch 1200: Train MSE = 0.000002\n",
            "Epoch 1201: Train MSE = 0.000002\n",
            "Epoch 1202: Train MSE = 0.000002\n",
            "Epoch 1203: Train MSE = 0.000002\n",
            "Epoch 1204: Train MSE = 0.000002\n",
            "Epoch 1205: Train MSE = 0.000001\n",
            "Epoch 1206: Train MSE = 0.000001\n",
            "Epoch 1207: Train MSE = 0.000001\n",
            "Epoch 1208: Train MSE = 0.000001\n",
            "Epoch 1209: Train MSE = 0.000001\n",
            "Epoch 1210: Train MSE = 0.000001\n",
            "Epoch 1211: Train MSE = 0.000001\n",
            "Epoch 1212: Train MSE = 0.000001\n",
            "Epoch 1213: Train MSE = 0.000001\n",
            "Epoch 1214: Train MSE = 0.000001\n",
            "Epoch 1215: Train MSE = 0.000001\n",
            "Epoch 1216: Train MSE = 0.000001\n",
            "Epoch 1217: Train MSE = 0.000001\n",
            "Epoch 1218: Train MSE = 0.000001\n",
            "Epoch 1219: Train MSE = 0.000001\n",
            "Epoch 1220: Train MSE = 0.000001\n",
            "Epoch 1221: Train MSE = 0.000001\n",
            "Epoch 1222: Train MSE = 0.000001\n",
            "Epoch 1223: Train MSE = 0.000001\n",
            "Epoch 1224: Train MSE = 0.000001\n",
            "Epoch 1225: Train MSE = 0.000001\n",
            "Epoch 1226: Train MSE = 0.000001\n",
            "Epoch 1227: Train MSE = 0.000001\n",
            "Epoch 1228: Train MSE = 0.000001\n",
            "Epoch 1229: Train MSE = 0.000001\n",
            "Epoch 1230: Train MSE = 0.000001\n",
            "Epoch 1231: Train MSE = 0.000001\n",
            "Epoch 1232: Train MSE = 0.000001\n",
            "Epoch 1233: Train MSE = 0.000001\n",
            "Epoch 1234: Train MSE = 0.000001\n",
            "Epoch 1235: Train MSE = 0.000001\n",
            "Epoch 1236: Train MSE = 0.000001\n",
            "Epoch 1237: Train MSE = 0.000001\n",
            "Epoch 1238: Train MSE = 0.000001\n",
            "Epoch 1239: Train MSE = 0.000001\n",
            "Epoch 1240: Train MSE = 0.000001\n",
            "Epoch 1241: Train MSE = 0.000001\n",
            "Epoch 1242: Train MSE = 0.000001\n",
            "Epoch 1243: Train MSE = 0.000001\n",
            "Epoch 1244: Train MSE = 0.000001\n",
            "Epoch 1245: Train MSE = 0.000001\n",
            "Epoch 1246: Train MSE = 0.000001\n",
            "Epoch 1247: Train MSE = 0.000001\n",
            "Epoch 1248: Train MSE = 0.000001\n",
            "Epoch 1249: Train MSE = 0.000001\n",
            "Epoch 1250: Train MSE = 0.000001\n",
            "Epoch 1251: Train MSE = 0.000001\n",
            "Epoch 1252: Train MSE = 0.000001\n",
            "Epoch 1253: Train MSE = 0.000001\n",
            "Epoch 1254: Train MSE = 0.000001\n",
            "Epoch 1255: Train MSE = 0.000001\n",
            "Epoch 1256: Train MSE = 0.000001\n",
            "Epoch 1257: Train MSE = 0.000001\n",
            "Epoch 1258: Train MSE = 0.000001\n",
            "Epoch 1259: Train MSE = 0.000001\n",
            "Epoch 1260: Train MSE = 0.000001\n",
            "Epoch 1261: Train MSE = 0.000001\n",
            "Epoch 1262: Train MSE = 0.000001\n",
            "Epoch 1263: Train MSE = 0.000001\n",
            "Epoch 1264: Train MSE = 0.000001\n",
            "Epoch 1265: Train MSE = 0.000001\n",
            "Epoch 1266: Train MSE = 0.000001\n",
            "Epoch 1267: Train MSE = 0.000001\n",
            "Epoch 1268: Train MSE = 0.000001\n",
            "Epoch 1269: Train MSE = 0.000001\n",
            "Epoch 1270: Train MSE = 0.000001\n",
            "Epoch 1271: Train MSE = 0.000001\n",
            "Epoch 1272: Train MSE = 0.000001\n",
            "Epoch 1273: Train MSE = 0.000001\n",
            "Epoch 1274: Train MSE = 0.000001\n",
            "Epoch 1275: Train MSE = 0.000001\n",
            "Epoch 1276: Train MSE = 0.000001\n",
            "Epoch 1277: Train MSE = 0.000001\n",
            "Epoch 1278: Train MSE = 0.000001\n",
            "Epoch 1279: Train MSE = 0.000001\n",
            "Epoch 1280: Train MSE = 0.000001\n",
            "Epoch 1281: Train MSE = 0.000001\n",
            "Epoch 1282: Train MSE = 0.000001\n",
            "Epoch 1283: Train MSE = 0.000001\n",
            "Epoch 1284: Train MSE = 0.000001\n",
            "Epoch 1285: Train MSE = 0.000001\n",
            "Epoch 1286: Train MSE = 0.000001\n",
            "Epoch 1287: Train MSE = 0.000001\n",
            "Epoch 1288: Train MSE = 0.000001\n",
            "Epoch 1289: Train MSE = 0.000001\n",
            "Epoch 1290: Train MSE = 0.000001\n",
            "Epoch 1291: Train MSE = 0.000001\n",
            "Epoch 1292: Train MSE = 0.000001\n",
            "Epoch 1293: Train MSE = 0.000001\n",
            "Epoch 1294: Train MSE = 0.000001\n",
            "Epoch 1295: Train MSE = 0.000001\n",
            "Epoch 1296: Train MSE = 0.000001\n",
            "Epoch 1297: Train MSE = 0.000001\n",
            "Epoch 1298: Train MSE = 0.000001\n",
            "Epoch 1299: Train MSE = 0.000001\n",
            "Epoch 1300: Train MSE = 0.000001\n",
            "Epoch 1301: Train MSE = 0.000001\n",
            "Epoch 1302: Train MSE = 0.000001\n",
            "Epoch 1303: Train MSE = 0.000000\n",
            "Epoch 1304: Train MSE = 0.000000\n",
            "Epoch 1305: Train MSE = 0.000000\n",
            "Epoch 1306: Train MSE = 0.000000\n",
            "Epoch 1307: Train MSE = 0.000000\n",
            "Epoch 1308: Train MSE = 0.000000\n",
            "Epoch 1309: Train MSE = 0.000000\n",
            "No improvement in 100 epochs. Stopping.\n",
            "Test MSE = 359.139233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ASGD case"
      ],
      "metadata": {
        "id": "W619gwRHH3Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusted ASGD training with correct target shapes\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "def asgd_worker(rank, model, optimizer, shared, train_dataset,\n",
        "                batch_size, max_epochs, max_no_improve, epsilon, device):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(max_epochs):\n",
        "        if shared.stop:\n",
        "            break\n",
        "        # one epoch of Hogwild SGD updates\n",
        "        for Xb, yb in train_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(Xb)         # preds.shape = [batch, 1]\n",
        "            loss = nn.MSELoss()(preds, yb)  # yb.shape = [batch, 1]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # rank 0 worker does early‐stop check\n",
        "        if rank == 0:\n",
        "            total_loss = 0.0\n",
        "            count = 0\n",
        "            for Xall, yall in DataLoader(train_dataset, batch_size=batch_size):\n",
        "                Xall, yall = Xall.to(device), yall.to(device)\n",
        "                with torch.no_grad():\n",
        "                    pred = model(Xall)\n",
        "                    total_loss += nn.MSELoss(reduction='sum')(pred, yall).item()\n",
        "                count += yall.size(0)\n",
        "            train_mse = total_loss / count\n",
        "            print(f\"[Epoch {epoch+1}] Train MSE = {train_mse:.6e}\")\n",
        "            if train_mse + epsilon < shared.best_loss:\n",
        "                shared.best_loss = train_mse\n",
        "                shared.epochs_no_improve = 0\n",
        "            else:\n",
        "                shared.epochs_no_improve += 1\n",
        "            if shared.best_loss < epsilon or shared.epochs_no_improve >= max_no_improve:\n",
        "                shared.stop = True\n",
        "    print(f\"Worker {rank} exiting\")\n",
        "\n",
        "\n",
        "def train_asgd_linear(lin_splits, workers=1, max_epochs=100000,\n",
        "                      max_no_improve=1000, epsilon=1e-6, device='cpu'):\n",
        "    # unpack & combine train+val, and UNIFY target shapes to [N,1]\n",
        "    X_tr, y_tr, X_val, y_val, X_te, y_te = lin_splits\n",
        "    X_comb = np.vstack([X_tr, X_val])\n",
        "    y_comb = np.concatenate([y_tr, y_val])[:, None]  # now shape (N,1)\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_comb).float(),\n",
        "                             torch.from_numpy(y_comb).float())\n",
        "    test_ds  = TensorDataset(torch.from_numpy(X_te).float(),\n",
        "                             torch.from_numpy(y_te[:, None]).float())\n",
        "\n",
        "    # compute eta_95\n",
        "    U, S, _ = np.linalg.svd(X_comb, full_matrices=False)\n",
        "    eta_95 = 0.95 * (2.0 / (S[0]**2))\n",
        "\n",
        "    # model & optimizer\n",
        "    input_dim = X_comb.shape[1]\n",
        "    model = nn.Linear(input_dim, 1, bias=True)\n",
        "    model.share_memory()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=eta_95)\n",
        "\n",
        "    # shared state\n",
        "    manager = mp.Manager()\n",
        "    shared = manager.Namespace()\n",
        "    shared.best_loss = float('inf')\n",
        "    shared.epochs_no_improve = 0\n",
        "    shared.stop = False\n",
        "\n",
        "    mp.set_start_method('fork', force=True)\n",
        "\n",
        "    batch_size = 20\n",
        "    processes = []\n",
        "    for rank in range(workers):\n",
        "        p = mp.Process(\n",
        "            target=asgd_worker,\n",
        "            args=(\n",
        "                rank, model, optimizer, shared,\n",
        "                train_ds, batch_size,\n",
        "                max_epochs, max_no_improve, epsilon, device\n",
        "            )\n",
        "        )\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "\n",
        "    # test evaluation\n",
        "    model.eval()\n",
        "    total_te = 0.0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in DataLoader(test_ds, batch_size=batch_size):\n",
        "            preds = model(Xb.to(device))\n",
        "            total_te += nn.MSELoss(reduction='sum')(preds, yb.to(device)).item()\n",
        "    test_mse = total_te / len(test_ds)\n",
        "    print(f\"\\nASGD Test MSE = {test_mse:.6e}\")\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    workers = 10\n",
        "    trained_asgd = train_asgd_linear(lin_splits, workers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSCOhXMFH5rR",
        "outputId": "6f610ebc-962b-4a6e-a229-272ff83cdd81"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train MSE = 6.161611e+02\n",
            "[Epoch 2] Train MSE = 2.806573e+02\n",
            "[Epoch 3] Train MSE = 1.898026e+02\n",
            "[Epoch 4] Train MSE = 8.408646e+01\n",
            "[Epoch 5] Train MSE = 2.402593e+01\n",
            "[Epoch 6] Train MSE = 1.411208e+01\n",
            "[Epoch 7] Train MSE = 6.994892e+00\n",
            "[Epoch 8] Train MSE = 4.270392e+00\n",
            "[Epoch 9] Train MSE = 2.833372e+00\n",
            "[Epoch 10] Train MSE = 1.907298e+00\n",
            "[Epoch 11] Train MSE = 1.282774e+00\n",
            "[Epoch 12] Train MSE = 9.800470e-01\n",
            "[Epoch 13] Train MSE = 7.313037e-01\n",
            "[Epoch 14] Train MSE = 5.415344e-01\n",
            "[Epoch 15] Train MSE = 3.803157e-01\n",
            "[Epoch 16] Train MSE = 2.975742e-01\n",
            "[Epoch 17] Train MSE = 2.258846e-01\n",
            "[Epoch 18] Train MSE = 1.780057e-01\n",
            "[Epoch 19] Train MSE = 1.427797e-01\n",
            "[Epoch 20] Train MSE = 1.152652e-01\n",
            "[Epoch 21] Train MSE = 8.474092e-02\n",
            "[Epoch 22] Train MSE = 6.770584e-02\n",
            "[Epoch 23] Train MSE = 5.234907e-02\n",
            "[Epoch 24] Train MSE = 4.034205e-02\n",
            "[Epoch 25] Train MSE = 3.192861e-02\n",
            "[Epoch 26] Train MSE = 2.625798e-02\n",
            "[Epoch 27] Train MSE = 2.196052e-02\n",
            "[Epoch 28] Train MSE = 1.788813e-02\n",
            "[Epoch 29] Train MSE = 1.402755e-02\n",
            "[Epoch 30] Train MSE = 1.043127e-02\n",
            "[Epoch 31] Train MSE = 7.867171e-03\n",
            "[Epoch 32] Train MSE = 6.604252e-03\n",
            "[Epoch 33] Train MSE = 5.565841e-03\n",
            "[Epoch 34] Train MSE = 4.363244e-03\n",
            "[Epoch 35] Train MSE = 3.665419e-03\n",
            "[Epoch 36] Train MSE = 3.050760e-03\n",
            "[Epoch 37] Train MSE = 2.410340e-03\n",
            "[Epoch 38] Train MSE = 1.836315e-03\n",
            "[Epoch 39] Train MSE = 1.493727e-03\n",
            "[Epoch 40] Train MSE = 1.264621e-03\n",
            "[Epoch 41] Train MSE = 1.071138e-03\n",
            "[Epoch 42] Train MSE = 9.162944e-04\n",
            "[Epoch 43] Train MSE = 7.209259e-04\n",
            "[Epoch 44] Train MSE = 5.615996e-04\n",
            "[Epoch 45] Train MSE = 4.449303e-04\n",
            "[Epoch 46] Train MSE = 3.282171e-04\n",
            "[Epoch 47] Train MSE = 2.511756e-04\n",
            "[Epoch 48] Train MSE = 2.044828e-04\n",
            "[Epoch 49] Train MSE = 1.645648e-04\n",
            "[Epoch 50] Train MSE = 1.221644e-04\n",
            "[Epoch 51] Train MSE = 9.546596e-05\n",
            "[Epoch 52] Train MSE = 7.639242e-05\n",
            "[Epoch 53] Train MSE = 6.598039e-05\n",
            "[Epoch 54] Train MSE = 5.229614e-05\n",
            "[Epoch 55] Train MSE = 4.122228e-05\n",
            "[Epoch 56] Train MSE = 3.294444e-05\n",
            "[Epoch 57] Train MSE = 2.635106e-05\n",
            "[Epoch 58] Train MSE = 2.105140e-05\n",
            "[Epoch 59] Train MSE = 1.650866e-05\n",
            "[Epoch 60] Train MSE = 1.354130e-05\n",
            "[Epoch 61] Train MSE = 1.009236e-05\n",
            "[Epoch 62] Train MSE = 9.067854e-06\n",
            "[Epoch 63] Train MSE = 8.086504e-06\n",
            "[Epoch 64] Train MSE = 6.930246e-06\n",
            "[Epoch 65] Train MSE = 5.839255e-06\n",
            "[Epoch 66] Train MSE = 4.610310e-06\n",
            "[Epoch 67] Train MSE = 3.729030e-06\n",
            "[Epoch 68] Train MSE = 3.084120e-06\n",
            "[Epoch 69] Train MSE = 2.473731e-06\n",
            "[Epoch 70] Train MSE = 2.094738e-06\n",
            "[Epoch 71] Train MSE = 1.877475e-06\n",
            "[Epoch 72] Train MSE = 1.558760e-06\n",
            "[Epoch 73] Train MSE = 1.223266e-06\n",
            "[Epoch 74] Train MSE = 9.690558e-07\n",
            "[Epoch 75] Train MSE = 8.103022e-07\n",
            "Worker 0 exiting\n",
            "Worker 8 exitingWorker 9 exitingWorker 4 exitingWorker 7 exiting\n",
            "\n",
            "\n",
            "\n",
            "Worker 1 exiting\n",
            "Worker 6 exitingWorker 3 exiting\n",
            "\n",
            "Worker 5 exitingWorker 2 exiting\n",
            "\n",
            "\n",
            "ASGD Test MSE = 3.563503e+02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Non-Linear Case Experiments:**\n",
        "  Training + evaluating all the different kind of regularization\n",
        "\n",
        "  - Baseline\n",
        "  - Dropout\n",
        "  - Weight Decay/L2\n",
        "  - Gradient Noise Injection\n",
        "  - ...."
      ],
      "metadata": {
        "id": "w6KvDCnINjgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "ISS8LfWhNrNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Helper: select activation\n",
        "def get_activation_fn(name: str):\n",
        "    name = name.lower()\n",
        "    if name == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif name == 'sigmoid':\n",
        "        return nn.Sigmoid()\n",
        "    elif name == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    elif name == 'linear':\n",
        "        return nn.Identity()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported activation. Choose from 'relu','sigmoid','tanh','linear'.\")\n",
        "\n",
        "# Linear regression model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, input_dim, bias=True):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1, bias=bias)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x).squeeze(-1)  # shape: (batch,)\n",
        "\n",
        "# MLP for nonlinear cases\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers=1, hidden_units=64, activation='relu', bias=True):\n",
        "        \"\"\"\n",
        "        input_dim: number of features\n",
        "        hidden_layers: number of hidden layers\n",
        "        hidden_units: units per hidden layer\n",
        "        activation: nonlinearity name\n",
        "        bias: whether to use bias terms\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        act_fn = get_activation_fn(activation)\n",
        "        layers = []\n",
        "        in_dim = input_dim\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(nn.Linear(in_dim, hidden_units, bias=bias))\n",
        "            layers.append(act_fn)\n",
        "            in_dim = hidden_units\n",
        "        layers.append(nn.Linear(in_dim, 1, bias=bias))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "# Training loop for regression (MSE)\n",
        "def train_model(model, optimizer, data_loader, epoch, num_epochs, grad_noise_std=0.0, device='cpu'):\n",
        "    model.train()\n",
        "    criterion = nn.MSELoss()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_batch)\n",
        "        loss = criterion(preds, y_batch)\n",
        "        loss.backward()\n",
        "        # optional gradient noise\n",
        "        if grad_noise_std > 0:\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters():\n",
        "                    if p.grad is not None:\n",
        "                        p.grad.add_(torch.randn_like(p.grad) * grad_noise_std)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "    return running_loss / len(data_loader.dataset)\n",
        "\n",
        "# Evaluation loop (MSE)\n",
        "def evaluate_model(model, data_loader, device='cpu'):\n",
        "    model.eval()\n",
        "    criterion = nn.MSELoss(reduction='sum')\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            preds = model(X_batch)\n",
        "            total_loss += criterion(preds, y_batch).item()\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Run experiment wrapper\n",
        "def run_experiment(name, train_data, val_data, test_data,\n",
        "                   model_ctor, model_kwargs,\n",
        "                   learning_rate=1e-3, weight_decay=0.0,\n",
        "                   batch_size=64, epochs=50, grad_noise_std=0.0,\n",
        "                   device='cpu'):\n",
        "    print(f\"\\n=== Experiment: {name} ===\")\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_data,   batch_size=batch_size, shuffle=False)\n",
        "    test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = model_ctor(**model_kwargs).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    start = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_model(model, optimizer, train_loader, epoch, epochs, grad_noise_std, device)\n",
        "        val_loss = evaluate_model(model, val_loader, device)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train MSE: {train_loss:.4f}, Val MSE: {val_loss:.4f}\")\n",
        "    duration = time.time() - start\n",
        "    print(f\"Training finished in {duration:.1f}s\")\n",
        "\n",
        "    test_loss = evaluate_model(model, test_loader, device)\n",
        "    print(f\"Test MSE: {test_loss:.4f}\")\n",
        "    return model, train_losses, val_losses, test_loss\n",
        "\n",
        "# ASGD (Hogwild!) worker\n",
        "def train_worker(rank, shared_model, optimizer, dataset, epochs, batch_size, grad_noise_std, device):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    shared_model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = nn.MSELoss()(shared_model(X_batch), y_batch)\n",
        "            loss.backward()\n",
        "            if grad_noise_std > 0:\n",
        "                with torch.no_grad():\n",
        "                    for p in shared_model.parameters():\n",
        "                        if p.grad is not None:\n",
        "                            p.grad.add_(torch.randn_like(p.grad) * grad_noise_std)\n",
        "            optimizer.step()\n",
        "        print(f\"Worker {rank} completed epoch {epoch+1}\")\n",
        "\n",
        "def run_asgd(model_ctor, model_kwargs, train_data, learning_rate=1e-3,\n",
        "             batch_size=64, workers=4, epochs=10, grad_noise_std=0.0, device='cpu'):\n",
        "    model = model_ctor(**model_kwargs)\n",
        "    model.share_memory()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    import torch.multiprocessing as mp\n",
        "    processes = []\n",
        "    epochs_per_worker = epochs\n",
        "    for rank in range(workers):\n",
        "        p = mp.Process(target=train_worker,\n",
        "                       args=(rank, model, optimizer, train_data, epochs_per_worker, batch_size, grad_noise_std, device))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "JkAPZuaSNmKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HqM-v7eWOKc0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_s8DEZDOK4n"
      },
      "source": [
        "## **Hyperparameter tuning for baseline**\n",
        "\n",
        "Functions have to be changed !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEcTiAhOOK4o"
      },
      "source": [
        "Using Keras for this because its interface is easier for this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "bb83b167-6580-4bdf-e69c-fb9356fd9abe",
        "id": "5rUTeIEmOK4o"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_core\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras_core) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras_core) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras_core) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras_core) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras_core) (3.13.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras_core) (0.1.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras_core) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras_core) (1.17.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras_core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras_core) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras_core) (0.1.2)\n",
            "Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras_core\n",
            "Successfully installed keras_core-0.1.7\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_core\n",
        "!pip install keras-tuner --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzG24LaAOK4p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "import keras_core as keras\n",
        "#########################################\n",
        "import tensorflow as tf\n",
        "#from tensorflow.keras.optimizers import Adadelta\n",
        "import keras\n",
        "#from tensorflow.keras import layers\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import regularizers\n",
        "#import tensorflow as tf\n",
        "#from tensorflow import keras\n",
        "#from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import clone_model\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_poly, y_train_poly, X_val_poly, y_val_poly, X_test_poly, y_test_poly = poly_splits"
      ],
      "metadata": {
        "id": "jPeqcIbvPXC7"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6adc1586-d4c0-45e2-c9d4-c7ca569b01bf",
        "id": "GxpQCmZeOK4r"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from /content/optML_mini_project/Hyperparam Search Baseline Model For Linear Overparametrized/tuner0.json\n"
          ]
        }
      ],
      "source": [
        "# Define the model as a function for Keras Tuner for regression\n",
        "def build_model(hp):\n",
        "    net = keras.Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    net.add(keras.layers.Input(shape=(X_train_poly.shape[1],)))\n",
        "\n",
        "    # Define the activation function to be used for all layers\n",
        "    activation_function = hp.Choice('activation_function', ['relu', 'tanh', 'sigmoid'])\n",
        "\n",
        "    # No regularization in base line\n",
        "    #regularization = hp.Float('regularization', min_value=0.0, max_value=0.1, step=0.005)\n",
        "\n",
        "    # Tune the number of hidden layers\n",
        "    for i in range(hp.Int('hidden_layers', 2, 5)):\n",
        "        # Tune the number of units per layer\n",
        "        units = hp.Int(f'units_in_layer{i}', min_value=32, max_value=256, step=32)\n",
        "        net.add(keras.layers.Dense(units=units, activation= activation_function, kernel_regularizer=regularizers.l2(0)))\n",
        "\n",
        "    # Output layer\n",
        "    net.add(keras.layers.Dense(units=1, activation= activation_function))\n",
        "\n",
        "    net.compile(\n",
        "        optimizer=keras.optimizers.SGD(learning_rate=hp.Float('learning_rate', 1e-3, 1e-1, sampling='log')),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "        )\n",
        "    return net\n",
        "\n",
        "# Define the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',  # Optimize for validation loss, not accuracy in regression\n",
        "    max_trials=100,         # Number of hyperparameter combinations to try\n",
        "    executions_per_trial=1, # Number of times to train each configuration\n",
        "    directory='/content/optML_mini_project',\n",
        "    project_name='Hyperparam Search Baseline Model For Non Linear Overparametrized'\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',       # Monitor validation loss for early stopping\n",
        "    patience=5,               # Number of epochs with no improvement before stopping\n",
        "    restore_best_weights=True # Restore model weights from the epoch with the best validation loss\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_J0hF46OK4r"
      },
      "source": [
        "**Only run this cell if want training/search again => THIS CAN TAKE A LONG TIME**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e27262-44db-4975-93cf-2f56045d12c2",
        "id": "W7hCLYcIOK4s"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 250 Complete [00h 00m 05s]\n",
            "val_loss: 1600.9865112304688\n",
            "\n",
            "Best val_loss So Far: 1409.7673950195312\n",
            "Total elapsed time: 00h 21m 31s\n"
          ]
        }
      ],
      "source": [
        "# Run tuner search with custom loss history and early stopping\n",
        "for trial_id in range(tuner.oracle.max_trials):\n",
        "    tuner.search(\n",
        "        X_train_poly, y_train_poly,\n",
        "        validation_data=(X_val_poly, y_val_poly),\n",
        "        epochs=50,\n",
        "        batch_size=20,\n",
        "        callbacks=[early_stopping]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1bbe16d-7a80-48f2-8e0c-dac1eb7669de",
        "collapsed": true,
        "id": "MnD189nPOK4t"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 1\n",
            "learning_rate (Float)\n",
            "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.1, 'step': None, 'sampling': 'log'}\n",
            "Results summary\n",
            "Results in /content/optML_mini_project/Hyperparam Search Baseline Model For Linear Overparametrized\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_loss\", direction=\"min\")\n",
            "\n",
            "Trial 045 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.010881800788534846\n",
            "Score: 1409.7673950195312\n",
            "\n",
            "Trial 028 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.016459331291952827\n",
            "Score: 1433.366943359375\n",
            "\n",
            "Trial 024 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.01411254804624754\n",
            "Score: 1434.1825561523438\n",
            "\n",
            "Trial 217 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.013233340167603112\n",
            "Score: 1439.9340209960938\n",
            "\n",
            "Trial 092 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.015256589990246058\n",
            "Score: 1443.3583984375\n",
            "\n",
            "Trial 179 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.001595935236110003\n",
            "Score: 1444.2477416992188\n",
            "\n",
            "Trial 242 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.01634224658884753\n",
            "Score: 1446.4083251953125\n",
            "\n",
            "Trial 056 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.011363871072843399\n",
            "Score: 1447.6484985351562\n",
            "\n",
            "Trial 004 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.01973462907005616\n",
            "Score: 1448.198486328125\n",
            "\n",
            "Trial 216 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.01172501649121111\n",
            "Score: 1453.911376953125\n"
          ]
        }
      ],
      "source": [
        "# Print the summary of the search space\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Print the results of the search\n",
        "tuner.results_summary()                       #WILL RETURN WRONG LAYER SIZES!!!! STARTS OF RIGHT, BUT FILLED WITH ADDITIONAL BS => check amount of layers it says there is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5zUsePTOK4t"
      },
      "source": [
        "Save results in GIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7166673-efb0-4ac9-c356-426ac7514d69",
        "id": "OBLvcqhVOK4u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\content\\optML_mini_project\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ],
      "source": [
        "%cd /content/optML_mini_project\n",
        "!git add --a\n",
        "!git commit -m \"Hyperparameter search for linear baseline model\"\n",
        "!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25db68ff-8593-4ff6-dc05-28c26a7c1250",
        "collapsed": true,
        "id": "sszxTJbnOK4u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 1 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 2 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 3 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 4 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 5 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 6 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 7 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 8 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 9 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 10 Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m501\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m501\u001b[0m (1.96 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">501</span> (1.96 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 1 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.010881801135838032\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n",
            "\n",
            "Model 2 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.016459330916404724\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n",
            "\n",
            "Model 3 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.014112547971308231\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n",
            "\n",
            "Model 4 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.013233340345323086\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n",
            "\n",
            "Model 5 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.015256590209901333\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n",
            "\n",
            "Model 6 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.001595935202203691\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n",
            "\n",
            "Model 7 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.016342246904969215\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n",
            "\n",
            "Model 8 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.011363871395587921\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n",
            "\n",
            "Model 9 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.01973462849855423\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n",
            "\n",
            "Model 10 Optimizer:\n",
            "  name: SGD\n",
            "  learning_rate: 0.011725016869604588\n",
            "  weight_decay: None\n",
            "  clipnorm: None\n",
            "  global_clipnorm: None\n",
            "  clipvalue: None\n",
            "  use_ema: False\n",
            "  ema_momentum: 0.99\n",
            "  ema_overwrite_frequency: None\n",
            "  loss_scale_factor: None\n",
            "  gradient_accumulation_steps: None\n",
            "  momentum: 0.0\n",
            "  nesterov: False\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the best model from the tuner\n",
        "top_models = tuner.get_best_models(num_models=10)\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Display summaries of the top models\n",
        "for i, model in enumerate(top_models, start=1):\n",
        "    print(f\"\\nModel {i} Summary:\")\n",
        "    model.summary()\n",
        "# Display the optimizer for each model\n",
        "for i, model in enumerate(top_models, start=1):\n",
        "    optimizer_config = model.optimizer.get_config()  # Get optimizer configuration\n",
        "    print(f\"\\nModel {i} Optimizer:\")\n",
        "    for key, value in optimizer_config.items():\n",
        "        print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8113Xh1IOK4v"
      },
      "source": [
        "If need to save the best model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gIqeF9dEGG2N",
        "W619gwRHH3Ue",
        "ISS8LfWhNrNH"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "OptiML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}